<!DOCTYPE html><html lang="ja"><head><meta charset="utf-8">
<title>CSS Speech Module Level 1（日本語訳）</title>

<link rel="stylesheet" href="common.css" type="text/css">
<link rel="stylesheet" href="common-css.css" type="text/css">

<style>
#aural-box {
	position: relative;
	display: block;
	overflow-y: hidden;
	margin: 0;
	padding: 0;
	border: none;
	height: 20.5em;
	line-height: 1em;
	white-space: nowrap;
}
#aural-box > li {
	position: absolute;
	display: block;
	padding: 0;
	margin: 0;
}

#aural-box > li > * {
	position: absolute;
}

#aural-box > li > *:nth-child(1) {
	left: 2.5em;
	top: -0.5em;
}

#aural-box > li > *:nth-child(2) {
	top:0;
	left:0;
	border: thin solid black;
	border-style: solid none none solid;
	width: 2em;
}

#aural-box > li > *:nth-child(3) {
	bottom:0;
	left:0;
	border: thin solid silver;
	border-style: none none solid solid;
	width: 2em;
}

#aural-box > li > *:nth-child(4) {
	left: 2.5em;
	bottom: -0.5em;
}

</style>
<script src="common0.js"></script>
<script src="common1.js" async></script>

<script>
Util.ready = function(){
	const source_data = {
		generate: expand,
	};
	Util.switchWordsInit(source_data);
}


function expand(){
	const class_map = this.class_map;
	const tag_map = this.tag_map;
	const link_map = this.link_map;

	let context_prop = '';

	return this.html.replace(
		/%[\w\-~一-鿆]+|`(.+?)([$@\^])(\w*)/g,
		create_html
	);

	function create_html(match, key, indicator, klass){
if(!indicator) {//%
	return `<var>${match.slice(1)}</var>`;
}

let href = '';
let href1 = '';
{
	const n = key.indexOf('＠');
	if(n > 0) {
		href1 = key.slice(n + 1);
		key = key.slice(0, n);
	}
}
let text = key;

switch(klass){
case 'r':
	text = `[${key}]`;
	href = `#biblio-${key.toLowerCase()}`;
	break;
case 't':
	text = `&lt;${key}&gt;`;
	key = key.replace(/\s?\[.+/, '');
	break;
case 'vv': 
	context_prop = `#valdef-${key}-`;
	return '';
	break;
case 'v': 
	href = context_prop + key;
	break;
case 'vt':
	text = `&lt;${key}&gt;`;
	key = key.replace(/\s?\[.+/, '');
	href = context_prop + key;
	break;
case 'tp':
	text = `&lt;'<code class="property">${key}</code>'&gt;`;
	href = link_map[`p.${key}`];
	break;
case 'f':
	text = `${key}()`;
	break;
case 'pe':
	text = `::${key}`;
	break;
case 'issue':
	text = `課題 #${key}`;
	href = `~CSSissue/${key}`;
	break;
case 'en':
	text = `<span lang="en">${key}</span>`;
	break;
}


let tag = tag_map[klass];
if(tag) {
	let classname = class_map[klass];
	classname = classname ? ` class="${classname}"` : '';
	text = `<${tag}${classname}>${text}</${tag}>`;
}

if(indicator !== '^'){
	href = href1 || link_map[ klass ? `${klass}.${key}` : key ] || href;
	if(!href){
		console.log(match); // check error
		return match;
	}

	switch(indicator){
	case '$':
		text = `<a href="${href}">${text}</a>`;
		break;
	case '@':
		text = `<dfn id="${href.slice(1)}">${text}</dfn>`;
		break;
	}
}

return text;

	}
}

</script>


<script type="text/plain" id="_source_data">


●●options

spec_title:CSS Speech Module
spec_date:2022-09-06
	real_date:2023-03-03
trans_update:2023-03-06
source_checked:180206
spec_status:ED
page_state_key:CSS
original_url:https://drafts.csswg.org/css-speech-1/
ref_id_prefix:biblio-
ref_id_lowercase:true
conformance:css
copyright:2023,permissive
trans_1st_pub:2013-10-30


●●class_map
p:property
f:func
t:type
tp:type
u:unit
vt:type
v:value
at:at-rule
pe:pseudo
P:production
css:css
e:element
sS:element
a:attr

●●tag_map
p:code
at:code
t:var
tp:var
vt:var
css:code
pe:code
f:code
d:code
c:code
e:code
sS:code
a:code
u:code
f:code
v:code
en:span
P:var
V:var
em:em

●●original_id_map
valdef-cue-before-decibel:
valdef-pause-before-time:valdef-pause-before-time-0s
valdef-rest-before-time:valdef-rest-before-time-0s
valdef-voice-duration-time:valdef-voice-duration-time-0s
valdef-voice-rate-percentage:valdef-voice-rate-percentage-0

●●link_map


	●at/pe/p/v/vt/f
pe.before:~CSSPSEUDO#selectordef-before
pe.after:~CSSPSEUDO#selectordef-after

p.voice-volume:#propdef-voice-volume
p.voice-balance:#propdef-voice-balance
p.speak:#propdef-speak
p.speak-as:#propdef-speak-as
p.pause-before:#propdef-pause-before
p.pause-after:#propdef-pause-after
p.pause:#propdef-pause
p.rest-before:#propdef-rest-before
p.rest-after:#propdef-rest-after
p.rest:#propdef-rest
p.cue-before:#propdef-cue-before
p.cue-after:#propdef-cue-after
p.cue:#propdef-cue
p.voice-family:#propdef-voice-family
p.voice-rate:#propdef-voice-rate
p.voice-pitch:#propdef-voice-pitch
p.voice-range:#propdef-voice-range
p.voice-stress:#propdef-voice-stress
p.voice-duration:#propdef-voice-duration

p.border:~CSSBG#propdef-border
p.content:~CSSCONTENT#propdef-content
p.display:~CSSDISP#propdef-display
p.font-family:~CSSFONT4#propdef-font-family
p.list-style-image:~CSSLIST#propdef-list-style-image
p.list-style-type:~CSSLIST#propdef-list-style-type
p.margin:~CSSBOX#propdef-margin
p.padding:~CSSBOX#propdef-padding
p.visibility:~CSSDISP#propdef-visibility

t.decibel:#typedef-voice-volume-decibel
t.generic-voice:#typedef-generic-voice
t.age:#typedef-voice-family-age
t.gender:#typedef-voice-family-gender
t.semitones:#typedef-voice-pitch-semitones

t.family-name:~CSSFONT4#family-name-value
t.frequency:~CSSVAL#frequency-value
t.string:~CSSVAL#string-value
t.custom-ident:~CSSVAL#identifier-value
	t.ident:~CSSVAL#typedef-ident
t.integer:~CSSVAL#integer-value
t.number:~CSSVAL#number-value
t.percentage:~CSSVAL#percentage-value
t.time:~CSSVAL#time-value
t.url:~CSSVAL#url-value
	t.uri:~CSS22/syndata.html#value-def-uri
f.counter:~CSSLIST#funcdef-counter

v.inherit:~CASCADE#valdef-all-inherit
	v.none:~CSSDISP#valdef-display-none
v.visible:~CSSOVERFLOW3#valdef-overflow-visible
	v.visible:~CSSDISP#valdef-visibility-visible

v.aural:~MQ5#valdef-media-aural
v.speech:~MQ5#valdef-media-speech
v.all:~MQ5#valdef-media-all
v.screen:~MQ5#valdef-media-screen

v.pronunciation:https://microformats.org/wiki/rel-pronunciation

	v.disc
	v.circle
	v.square

v.decimal:~CSSCOUNTER#decimal
v.decimal-leading-zero:~CSSCOUNTER#decimal-leading-zero
v.lower-roman:~CSSCOUNTER#lower-roman
v.lower-latin:~CSSCOUNTER#lower-latin
v.lower-alpha:~CSSCOUNTER#lower-alpha
v.lower-greek:~CSSCOUNTER#lower-greek
v.upper-roman:~CSSCOUNTER#upper-roman
v.upper-latin:~CSSCOUNTER#upper-latin
v.upper-alpha:~CSSCOUNTER#upper-alpha
v.georgian:~CSSCOUNTER#georgian
v.armenian:~CSSCOUNTER#armenian

	v.silent:#valdef-voice-volume-silent
	v.x-soft:#valdef-voice-volume-x-soft
	v.soft:#valdef-voice-volume-soft
	v.medium:#valdef-voice-volume-medium
	v.loud:#valdef-voice-volume-loud
	v.x-loud:#valdef-voice-volume-x-loud
	vt.decibel:#valdef-voice-volume-decibel

	vt.number:#valdef-voice-balance-number
	v.left:#valdef-voice-balance-left
	v.center:#valdef-voice-balance-center
	v.right:#valdef-voice-balance-right
	v.leftwards:#valdef-voice-balance-leftwards
	v.rightwards:#valdef-voice-balance-rightwards

	v.auto:#valdef-speak-auto
	v.never:#valdef-speak-never
	v.always:#valdef-speak-always

	v.normal:#valdef-speak-as-normal
	v.spell-out:#valdef-speak-as-spell-out
	v.digits:#valdef-speak-as-digits
	v.literal-punctuation:#valdef-speak-as-literal-punctuation
	v.no-punctuation:#valdef-speak-as-no-punctuation

	vt.time:#valdef-pause-before-time
	v.none:#valdef-pause-before-none
	v.x-weak:#valdef-pause-before-x-weak
	v.weak:#valdef-pause-before-weak
	v.medium:#valdef-pause-before-medium
	v.dfn:#valdef-pause-before-dfn
	v.x-dfn:#valdef-pause-before-x-dfn

	vt.time:#valdef-rest-before-time
	v.none:#valdef-rest-before-none
	v.x-weak:#valdef-rest-before-x-weak
	v.weak:#valdef-rest-before-weak
	v.medium:#valdef-rest-before-medium
	v.dfn:#valdef-rest-before-dfn
	v.x-dfn:#valdef-rest-before-x-dfn

	vt.uri:#valdef-cue-before-uri

	vt.family-name:#valdef-voice-family-family-name
	v.child:#valdef-voice-family-child
	v.young:#valdef-voice-family-young
	v.old:#valdef-voice-family-old
	v.male:#valdef-voice-family-male
	v.female:#valdef-voice-family-female
	v.neutral:#valdef-voice-family-neutral
	vt.integer:#valdef-voice-family-integer
	v.preserve:#valdef-voice-family-preserve

	v.normal:#valdef-voice-rate-normal
	v.x-slow:#valdef-voice-rate-x-slow
	v.slow:#valdef-voice-rate-slow
	v.medium:#valdef-voice-rate-medium
	v.fast:#valdef-voice-rate-fast
	v.x-fast:#valdef-voice-rate-x-fast
	vt.percentage:#valdef-voice-rate-percentage

	vt.frequency:#valdef-voice-pitch-frequency
	v.absolute:#valdef-voice-pitch-absolute
	vt.semitones:#valdef-voice-pitch-semitones
	vt.percentage:#valdef-voice-pitch-percentage
	v.x-low:#valdef-voice-pitch-x-low
	v.low:#valdef-voice-pitch-low
	v.medium:#valdef-voice-pitch-medium
	v.high:#valdef-voice-pitch-high
	v.x-high:#valdef-voice-pitch-x-high

	v.absolute:#valdef-voice-range-absolute
	vt.semitones:#valdef-voice-range-semitones
	vt.percentage:#valdef-voice-range-percentage
	v.x-low:#valdef-voice-range-x-low
	v.low:#valdef-voice-range-low
	v.medium:#valdef-voice-range-medium
	v.high:#valdef-voice-range-high
	v.x-high:#valdef-voice-range-x-high

	v.normal:#valdef-voice-stress-normal
	v.none:#valdef-voice-stress-none
	v.moderate:#valdef-voice-stress-moderate
	v.strong:#valdef-voice-stress-strong
	v.reduced:#valdef-voice-stress-reduced

	v.auto:#valdef-voice-duration-auto
	vt.time:#valdef-voice-duration-time


e.link:~HEmetadata#the-link-element
	e.span:~HEtextlevel#the-span-element

sS.sub:~SSML11#edef_sub
sS.audio:~SSML11#edef_audio
sS.break:~SSML11#edef_break
sS.prosody:~SSML11#edef_prosody
sS.emphasis:~SSML11#edef_emphasis
sS.say-as:~SSML11#edef_say-as
sS.voice:~SSML11#edef_voice


	●用語
聴覚-次元:#_aural-dimension
聴覚-~box~model:#aural-box-model
視覚-~box~model:~CSSBOX#box-model
半音:#voice-pitch-semitone

音声~cue:#cue-props
~pauseの相殺:#collapsed-pauses
~pause:#pause-props
~rest:#rest-props
	§:#aural-model
~SSML:#biblio-ssml

	●用語（CSS
~CSS全域~keyword:~CSSVAL#css-wide-keywords
~CSS識別子:~CSSVAL#css-css-identifier
次元:~CSSVAL#dimension
内容~言語:~CSSTEXT#content-language
媒体~型:~MQ5#media-type
~list~item:~CSSLIST#list-item
	~marker:~CSSLIST#marker
根~要素:~CSSDISP#root-element


●●words_table1
SSML11:https://www.w3.org/TR/speech-synthesis11/

●●words_table

	●声／音／話
SSML:
TTS:
	TTS:text-to-speech
音:sound::~
音響:sound::~
	大きさの無い音::no sound
発声-:announce::~::アナウンス
cue::::キュー
	指示音
聴覚-:aural::~
	aurally
	視覚-:visual
聴取れる:audible な::聴き取れる
	それに伴う音声節:relevant audible passages
聴感:auditory::聴覚
聴感上の:acoustic::~
音声化-:aural に render::~
音声化:aural-rendering::~
音声化n:rendition::音声化
	~~声に出され:aloud
発話-:speak::~
発話:speech::~
	spoken
発話用:speaking::~
発話速度:speaking rate::~
速度:rate::~
読上げら:read out:読み上げら
読まれ:read され:~
合成-:synthesize::~
合成:synthesis::~
合成な:syntheticな::~
	:post-synthesis
合成器:synthesizer::~

知覚-:perceive:~
	知覚し得る:perceivable
bell:::ベル
	聴き分け:discernible

聴感音量:loudness::~::ラウドネス
	強めの:-loud
出力音量:volume::~::ボリューム
peak:::ピーク
decibel:::デシベル
無音:silence::~
無音に:silent に::~
	silently
尺度:scale:~
	尺度／音階:
利得:ゲイン:gain::~
平均:average:~
mixing:::ミキシング
対数-:logarithmic:~

聴取者:listener::聴き取り者::リスナー
聴取:listening::聴き取り::リスニング
	聴き手
	聴取者から見て:relative to the listener’ position
	周囲:around listener position

背景雑音:background noise::~::バックグラウンドノイズ
	静かな:quiet な
	騒がしい:noisy な
	環境下:context
再生-:play:~
再生:playback:~
	再生-時:playing
録音:recording:~
録音-:record:~
	録音-済み:pre-recorded
	〜から（鳴らされ）::coming from
	強めの:-loud

	●音質
音階:scale::~::スケール
半音:semitone::~
半音数:semitones::~
	~~半音による等分平均律〜音階::equal temperament chromatic scale
dynamic-range:dynamic range:::ダイナミックレンジ
clipping-threshold::::クリッピング限界
spectrum::::スペクトル
特性:characteristics::~
歪み:distortion:~
忠実さ:fidelity:~
音符:note::~::ノート
基本周波数:fundamental frequency::~
周波数:frequency::~

	●音（時間／空間／次元
三次元:three-dimensional:~
側面方向:lateral:~
仰角:elevation::~
	隔てられ:separated
分布:distribution::~
信号:signal::~
圧縮-:compress:~
圧縮:compression::~
振幅:amplitude::~
増幅:amplification::~
増幅-:amplify::~
減衰-:attenuate::~
減衰:attenuation::~
波形:waveform::~
可変度:variability:~
変動範囲:variation:~
変動-:oscillate:~

timing::::タイミング
	不慮のタイムラグ::unexpected lag
lag::::タイムラグ
時間的:temporal::~
時間:time::~
区間:interval::~
時区間:time interval::~
持続時間:duration::~
rest:
pause:
音高:pitch::~::ピッチ
	間／ためらい
	より音高が高い::~higher-pitched
	高い／広い:high
Hertz::Hz
kiloHertz::kHz
位相変位:phase shifting::~::フェイズシフティング

stage::::ステージ
	音響ステージ::sound stage
	音声ステージ::audio stage
		（音場は sound field）
	音響システム::sound system
surround::::サラウンド
	サラウンド音響::surround sound
方位角:azimuth::~
mono::::モノ
mono-aural::::モノラル
stereo::::ステレオ
	stereo-capable
中央:center::~::センター
	地点／点:point
	位置:position
	中央に均等化::centered equalization
空間的:spatial:~

	●声／抑揚
voice::::ボイス
	声音／話声／人声／ボイス／人工声音／有声音
年齢:age:~
年齢層:age category:~
方言:dialect::~
	方言による差異を吸収::cater for dialectic variations
性別:gender::~
男性:male::~
中性:neutral::~
女性:female::~
少年:young boy::~
子供の:child:~
大人の:adult:~
単調:monotonic:~
	話声
	読み綴る::spell
	読み綴る::spell out
	綴り::spellings
音素:phoneme::~
音標:phonetic::~
	音標による記法:phonetic notation
発音:pronunciation::~
発音-:pronounce::~
	発音上の:phonetic::~
調子:tone:~
抑揚:intonation::~::イントネーション
屈折:inflection::~::インフレクション
lexicon:::辞書
	発音辞書
強勢:emphasis::~
強勢-:emphasize::~
	瞬時の~~強勢:stress
	全く強勢されない::totally de-emphasized
強度:strength::~
韻律:prosodic::~
accent::::アクセント
	~accent付き:accented
	~accentなしの:unaccented
活発:animated:~


	●文字／構文／値／単位
感嘆符:exclamation mark:~
実数:number:~
alphabet::::アルファベット
	~alphabet式::alphabetic
略語:abbreviation:~
頭字語:acronyms:~
bullet::::ビュレット
hash::::ハッシュ
一字:letter:~
	一字ごとに:letter-by-letter／one letter at a time
括弧類:braces:~
約物:punctuation:~
	約物:punctuation characters
forward-slash:forward slash:::スラッシュ
	度:degrees
比率:ratio:~
割合分:fraction:~
引用符付き:quoted:~
引用符付きに:quote:~
引用符無し:unquoted:~
mmlli::::ミリ
	~mmlli秒:milliseconds

	数:number
	数値:numerical value
	数値的に:numerically
	4 分の 1:quarter
	2 乗:squares
	2 の 12 乗根:twelfth root of two
	2 倍:twice
	秒:seconds

	●CSS
相殺-:collapse:~
相殺:collapsing:~
深さ:depth:~
基底線:baseline::~::ベースライン

	●仕様
普遍的:universal:~
利用能:usability:~
参考:informative:~
条件付き:conditional:~
有効:effective:~
柔軟:flexible:~
違法:illegal:~
	合法的に:legitimately
業界:industry-wide:~
通常時:normal:~
異論:objections:~
慣行:convention:~
強制的:forceful:~
精巧:sophisticated:~
特殊性:specificities:~
標準的:standard:~
冗漫さ:verbosity:~
冗長さ:redundancy:~
論題:topic:~
現実的:realistic:~
論題:topic:~

指名-:designate:~
付随-:accompany:~
提起-:raise:~
関係度:relationship:~
相関-:relate:~
相関:relation:~
明確さ:clarity:~
協同:cooperation:~
準拠-:comply:~
	準拠する:compliant
作成者:creator:~
明白:obvious:~

	〜を高める:contribute to increasing 〜
	〜能力がある:-capable
	能力がある:capable of
	一貫性が損なわれ:inconsistencies:~
	〜そうにない:unlikely
	見込まれる:likely
	必要に応じて:wherever necessary
	きっかり:exactly
	事実，::in fact
	featuring
	著名:famous
	〜とは対照的に:opposed
	逆に:opposite
	逆に，:conversely
	~~選択肢:list of possibilities
	可能性:possibility
	生じ得る:any potential
	備える:prepare
	~~相応:pretty
	を想定する:we refer to 〜
	概ね:roughly
	排他的:mutually-exclusive
	不一致:discrepancies
	そのため、:consequently
	ある程度の:certain degree of
	にもかかわらず:despite
	そのまま:as-is
	傾向にある:tend
	何故なら:because
	十分:enough
	〜し易く:ease
	稀:rare
	ほどよく:reasonable 〜 well／reasonably
	例として:For instance
	適した:suitable
	支持を受けて:in favor of
	たまたま:happen to
	示す:illustrates
	であろう:seems
	:although
	きちんと:well-
	視野から外れる:out-of-scope
	模倣-:emulate
	容認し得る:tolerable
	できるようにする:enable
	〜たい:wish
	ないようにする:avoid
	なり得るものと:potentially
	書かれた時点::At the time of writing
	記し-::written
	〜とする:assume
	備えて
	抵触する:breaking
	知らせる:know about
	知識を持ち得る:knowledgeable
	としても知られている:known as
	もたらす:lead to
	制作
	外れる:deviate
	から決まる:as dictated by
	作り直し:re-work
	-:achievable
	-:achieve
	試みる:try
	伝える:inform
	満たす:meet
	留意すべき:notable:~
	注:note
	〜しないようにする:prevent
	より精緻化-:refine the level of precision
	〜に関する:with regards to
	問わず:irrespective:~
	ままにされ:remain
	〜に留意:reminder…
	緩められ:loose
	〜としても知られる::referred-to
	併用:used together with
	支援-:assistance in
	方々:member
	綿密:thorough

	●処理／演算（一般
近似的:approximate:~
	およそ／約:approximately
	ある程度の誤差と引き換えに:albeit at the cost of a certain degree of approximation
	作動中でない~~状態::deactivated
加法的:additive:~
	合算される:take additively
	:combined additively
切詰めら:clamp さ::切り詰めら
切上げて:clamp して::切り上げて
切上げる:clamp する::切り上げる
切上げら:clamp さ::切り上げら
切下げる:clamp する::切り下げる
優先度:priority:~
	優先度~付きの:prioritized
	優先される:takes precedence
減分:decrement:~
	-:monotonically non-decreasing
増分:increment:~
	増大／拡大::increases
	non-decreasing (conceptually increasing)
前処理:preprocessing:~
等式:equation:~
等価性:equivalence:~

	後処理:post-
	評価し直-:re-evaluate
	再~評価:re-evaluation
	等しい:equal
	計算し直-:re-calculate
	比較-時:comparing
	比較的:comparably

	●未分類（動詞

連接-:adjoin::~
連接:adjoining::~
clip::::クリップ
downgrade::::ダウングレード
共存-:coexist:~
選択-:select:~
選択:selection:~
併合-:merge:~
付番:numbering:~
付番方式:numbering scheme:~
固定的:fixed:~
割振られ:allocate され:割り振られ

	結付け:associations
	区切られ:delimited
	区別する:disambiguate
	ごとに変わり得る:varies across
	取り込む:importing
	~access不能:inaccessible
	付与され:marked
	該当する:match
	生成-済み:pre-generated
	予約-済み:reserved
	要素:selected element
	要素の内容~言語:the language of the selected content
	複数の:duplicated or different
	異なる:different
	各種:different
	変わり:vary
	特能として様々な:featuring varying
	様々な:various
	読み取り:reading
	読み取る／書き出す:reads and/or writes
	与えられ:given
	習得:learning
	指南:teaching
	捉えられる／眺めて:seen
	真上から眺めて:seen from the top
	伴われる:pertaining
	揃え:alignment
	緩和-:alleviate
	選ばれ:chosen
	現れる:appear
	~layout:arrangement
	呼ばれ:called
	含む:contain
	含め:include
	対象外:not covered
	行われる:take place
	渡-:pass
	欠いて:lack
	挙げられ:listed
	在る:located at
	-:performed
	補われ:complemented
	前に〜が補われる:prefixed
	量られ:quantified as
	囲まれた:surrounded
	切り替え時:switching
	占める:takes up
	一部になる:take part
	見つからない:missing
	-:excluding

	●未分類
	HTML:(x)HTML5
	比して::relative to
glyph::::グリフ
digital::::デジタル
	~digital化:digitize
screen-reader:screen reader:::スクリーンリーダ
逐語的:literal:~

時計回りの:clockwise:~
反時計回りの:counter-clockwise:~
	下限／上限:minimum and maximum boundaries
	脱落:drop
	より速く:faster
深層:深い層
	深層へ伝わる:get carried down
	近い:close
	近く:close to
	近く:closely
	近付く::be closer to
	隣り合う:consecutive
	継ぎ目無く:seamless に
	よりゆっくり:slower
	最終結果:end-result
codec::::コーデック
	intrinsically
較正-:calibrate::~
	-calibrated
自然言語:language°:~
French:::フランス語
English:::英語
	English language
外国語:foreign:~
	人:people
個人:person:~
個人的:personal:~
個性:personality:~
身体能力:physical ability:~
monologue::::モノローグ
朗読:declamation:~
headphone::::ヘッドフォン
	a pair of headphones
speaker::::スピーカ
混成:mixed:~
混合-:mix:~
修飾:modifier:~
	段階:step:~
構造-:structural::~
変種:variant:~
	累積され:combined multiplicatively
文化的:cultural:~
生物学的:biological:~
言語的:linguistic:~
錯覚効果:illusion:~
線形:linear:~
	線形に比例するように:linearly-proportional manner

	印字不能な~~状況:print-disabled
	工業用:industrial
	医療用:medical
	車の運転時:driving vehicle
	年少者:young children
	在宅鑑賞:home entertainment
	周囲 360 度 平面:envisioned 360 degrees plane
	背後:behind
	低視力者:blind, visually-impaired
	電子本:e-book
	思慮分別:discretion
	図書館:library
	夜中の読書:night-reading
	穏やかな音楽:soft music

	繰り返され:repeated
	依然として:still
	からなる:setup
	連結:joining
	種類:kinds
	程度:level
	行:line
	自体が無かった:no manifestation 〜  at all
	重み:substantial
	ばらつきが生じ得る:may be discrete variations
	高度に:highly
	2 個:two
	〜の並び:sequence

	●指示語
phrase::::フレーズ
見出し:heading:~
	あたりの語数::words per
隣接な:adjacentな:隣接する
	文:sentence
	片:piece
	部分:part
	節:section
	左:left
	右:right
	右手側:right hand side
	左手側:left hand side
	右側:right side
	左側:left side
	:left-right
	左右軸:left-right axis
	左右間:left-right
	左右端:left and right extremities
	左右:left and right
	左右両側:left and right sides
	正面:in front of
	中間:intermediary
	点在する:interspersed
	端:extremity
	直後に／続く:immediately
	〜から〜まで:inclusive
	より大きい:greater
	より小さい:smaller
	最も内側の:innermost
	幅広い:broad variety
	全〜:full
	全く:totally
	全くの:total
	単に:merely
	大きく:greatly
	費やす:how long
	周辺:around
	どこであれ:At any point
	またがる:across
	以前に／予め:previously
	〜に先立って:prior
	同じになる:identical
	現在:currently
	以前の:earlier
	半分:half:~
	次の:next:~
	最も外側の:outermost
	非:non
	ずっと:much more
	1:~multiplicatively
	〜倍:multiplied
	毎分あたり:per minute
	いくつもの:numerous
	大きな:major
	低／狭い:low
	より低い:lower
	最も長い:longest
	周辺／近い:near
	ときには:sometimes
	一方:whereas
	併用:altogether
	の中で:amongst
	:become
	:in cases where
	:causes
	:during
	:end
	neither
	nor
	per
	plus
	pre
	:show
	-:span
	1:~tell
	:whereby
	:while
	-:whilst
	なる:yields


●●ref_data
SSML=主    www.asahi-net.or.jp/~ax2s-kmtn/ref/accessibility/REC-speech-synthesis11-20100907.html
SSML=・    ~TR/speech-synthesis11/

●●ref_key_map
CSS3GENCON:CSSCONTENT3
CSS3LIST:CSSLISTS3

●●ref_normative

[COMPOSITING-2]
    Compositing and Blending Level 2 URL: https://drafts.fxtf.org/compositing-2/ 
[CSS-BACKGROUNDS-3]
    Bert Bos; Elika Etemad; Brad Kemper. CSS Backgrounds and Borders Module Level 3. 22 December 2020. CR. URL: https://www.w3.org/TR/css-backgrounds-3/ 
[CSS-BOX-4]
    Elika Etemad. CSS Box Model Module Level 4. 21 April 2020. WD. URL: https://www.w3.org/TR/css-box-4/ 
[CSS-CASCADE-5]
    Elika Etemad; Miriam Suzanne; Tab Atkins Jr.. CSS Cascading and Inheritance Level 5. 8 June 2021. WD. URL: https://www.w3.org/TR/css-cascade-5/ 
[CSS-COUNTER-STYLES-3]
    Tab Atkins Jr.. CSS Counter Styles Level 3. 14 December 2017. CR. URL: https://www.w3.org/TR/css-counter-styles-3/ 
[CSS-DISPLAY-3]
    Tab Atkins Jr.; Elika Etemad. CSS Display Module Level 3. 18 December 2020. CR. URL: https://www.w3.org/TR/css-display-3/ 
[CSS-FONTS-3]
    John Daggett; Myles Maxfield; Chris Lilley. CSS Fonts Module Level 3. 20 September 2018. REC. URL: https://www.w3.org/TR/css-fonts-3/ 
[CSS-FONTS-4]
    John Daggett; Myles Maxfield; Chris Lilley. CSS Fonts Module Level 4. 17 November 2020. WD. URL: https://www.w3.org/TR/css-fonts-4/ 
[CSS-POSITION-3]
    Elika Etemad; et al. CSS Positioned Layout Module Level 3. 19 May 2020. WD. URL: https://www.w3.org/TR/css-position-3/ 
[CSS-PSEUDO-4]
    Daniel Glazman; Elika Etemad; Alan Stearns. CSS Pseudo-Elements Module Level 4. 31 December 2020. WD. URL: https://www.w3.org/TR/css-pseudo-4/ 
[CSS-SYNTAX-3]
    Tab Atkins Jr.; Simon Sapin. CSS Syntax Module Level 3. 16 July 2019. CR. URL: https://www.w3.org/TR/css-syntax-3/ 
[CSS-VALUES-3]
    Tab Atkins Jr.; Elika Etemad. CSS Values and Units Module Level 3. 6 June 2019. CR. URL: https://www.w3.org/TR/css-values-3/ 
[CSS-VALUES-4]
    Tab Atkins Jr.; Elika Etemad. CSS Values and Units Module Level 4. 11 November 2020. WD. URL: https://www.w3.org/TR/css-values-4/ 
[CSS2]
    Bert Bos; et al. Cascading Style Sheets Level 2 Revision 1 (CSS 2.1) Specification. 7 June 2011. REC. URL: https://www.w3.org/TR/CSS21/ 
[CSS3GENCON]
    Elika Etemad; Dave Cramer. CSS Generated Content Module Level 3. 2 August 2019. WD. URL: https://www.w3.org/TR/css-content-3/ 
[CSS3LIST]
    Elika Etemad; Tab Atkins Jr.. CSS Lists and Counters Module Level 3. 17 November 2020. WD. URL: https://www.w3.org/TR/css-lists-3/ 
[INFRA]
    Anne van Kesteren; Domenic Denicola. Infra Standard. Living Standard. URL: https://infra.spec.whatwg.org/ 
[MEDIAQUERIES-5]
    Dean Jackson; Florian Rivoal; Tab Atkins Jr.. Media Queries Level 5. 31 July 2020. WD. URL: https://www.w3.org/TR/mediaqueries-5/ 
[RFC2119]
    S. Bradner. Key words for use in RFCs to Indicate Requirement Levels. March 1997. Best Current Practice. URL: https://tools.ietf.org/html/rfc2119 
[SSML]
    Daniel Burnett; Zhi Wei Shuang. Speech Synthesis Markup Language (SSML) Version 1.1. 7 September 2010. REC. URL: https://www.w3.org/TR/speech-synthesis11/ 
[XML11]
    Tim Bray; et al. Extensible Markup Language (XML) 1.1 (Second Edition). 16 August 2006. REC. URL: https://www.w3.org/TR/xml11/ 

●●ref_informative

[HTML]
    Anne van Kesteren; et al. HTML Standard. Living Standard. URL: https://html.spec.whatwg.org/multipage/ 
[PRONUNCIATION-LEXICON]
    Paolo Baggia. Pronunciation Lexicon Specification (PLS) Version 1.0. 14 October 2008. REC. URL: https://www.w3.org/TR/pronunciation-lexicon/ 
[SSML-SAYAS]
    Daniel Burnett; et al. SSML 1.0 say-as attribute values. 26 May 2005. NOTE. URL: https://www.w3.org/TR/ssml-sayas/ 


●●trans_metadata
<p>
~THIS_PAGEは、~W3Cにより編集者草案として公開された
<a href="~SPEC_URL">CSS Speech Module</a>
を日本語に翻訳したものです。
~PUB
</p>

●●spec_metadata

最新公表バージョン
	https://www.w3.org/TR/css-speech-1/
公表履歴
	https://www.w3.org/standards/history/css-speech-1

テスト一式
	https://test.csswg.org/suites/css-speech-1_dev/nightly-unstable/
フィードバック
	<a href="https://github.com/w3c/csswg-drafts/labels/css-speech-1">CSSWG Issues Repository</a>

編集
	<a href="mailto:lwatson@tetralogical.com">Léonie Watson</a> (Tetralogical)
	<a href="http://fantasai.inkedblade.net/contact">Elika J. Etemad / fantasai</a> (Invited Expert)
前任編集者
	<a href="mailto:dweck@daisy.org">Daniel Weck</a> (DAISY Consortium)
	Claudio Santambrogio (Opera Software)
	<a href="mailto:dsr@w3.org">Dave Ragett</a> (W3C / Canon)

Suggest an Edit for this Spec:
	<a href="https://github.com/w3c/csswg-drafts/blob/main/css-speech-1/Overview.bs">GitHub Editor</a>

commit 履歴
	https://github.com/w3c/csswg-drafts/commits/main/css-speech-1

</script>

</head>

<body>

<header>
	<hgroup>
<h1 id="top">CSS Speech Module</h1>
	</hgroup>
</header>

<div id="MAIN" hidden>

	<section id="abstract">
<h2 title="Abstract">要約</h2>

<p>
Speech ~moduleは、
いくつかの~CSS聴覚-~propを定義する
— それらは、
作者が［
発話~合成を通して, あるいは~optionの音声~cueを利用して，
文書の音声化（ `rendering^en ）を宣言的に制御する
］ことを可能化する。
この標準は、
`Voice Browser Activity＠https://www.w3.org/Voice/$en
との協同で開発された。
◎
The Speech module defines aural CSS properties that enable authors to declaratively control the rendering of documents via speech synthesis, and using optional audio cues. Note that this standard was developed in cooperation with the Voice Browser Activity.
</p>

~CSSisaLANG

	</section>
	<section id="sotd">
<h2 title="Status of this document">この文書の位置付け</h2>

<p>
これは、
編集者草案の公な複製です…
【以下、この節の他の内容は，~SOTD-CSSに移譲。】
</p>

<p id="at-risk">
次に挙げる特能は~risk下にあり、
勧告候補の期間に落とされるかもしれません：
◎
The following features are at-risk, and may be dropped during the CR period:
• voice-balance, voice-duration, voice-pitch, voice-range, and voice-stress 
</p>

<ul ><li>`voice-balance$p
</li><li>`voice-duration$p
</li><li>`voice-pitch$p
</li><li>`voice-range$p
</li><li>`voice-stress$p
</li></ul>


	</section>

<main id="MAIN0">

	<section id="_translation-of-terms_">
<h2>【用語の対訳】</h2>

<p>
この仕様で特に用いられている主な語彙の対訳を以下に示す。
一部の語は原語そのままにしている。
これらは，主に、
他の（特に視覚的な）~CSS関連の仕様には見られない語であって, かつ
この仕様が定義する語ではなく, かつ
原語が自明でないと考えられる語†として挙げている
<small>（†
次に挙げるような理由で
—
一般に目にする機会が少ない／
定訳がない／
定訳はあるが他の語の対訳としてもあり得る／
定訳と異なる対訳を用いている／
外来語として記される方が多い ／
等々）</small>：
</p>

<table><thead><tr><th style="min-width:7em">対訳
</th><th style="min-width:10em">原語
</th><th>備考
</th></tr></thead>

<tbody><tr><td>~voice
</td><td>`voice^en
</td><td>
“声”（人の声を模する音）。
視覚-媒体における~fontに相当する。

</td></tr><tr><td>音声
</td><td>`audio^en
</td><td>

</td></tr><tr><td>音声化する／音声化（体言）／〜による音声化n
</td><td>
`render^en, `rendered aurally^en ／ `rendering^en, `aural rendering^en ／ `rendition^en
</td><td>
聴覚-媒体に具現化する

</td></tr><tr><td>発話する／発話（体言）
</td><td>`speak^en ／ `speech^en
</td><td>
“読み上げ” と対訳される方が多いと思われるが，語義／語幹の観点から “話” を入れた語を利用する。
（ “読み上げ” は “`read out^en” などの対訳に利用される。）

</td></tr><tr><td>発話速度
</td><td>`speaking rate^en
</td><td>

</td></tr><tr><td>発話~合成／合成な発話
</td><td>`speech synthesis^en／`synthetic speech^en
</td><td>

</td></tr><tr><td>発話~合成器
</td><td>`speech synthesizer^en
</td><td>

</td></tr><tr><td>~TTS
</td><td>`text-to-speech^en, 略して `TTS^en
</td><td>
発話~合成~用の［
技術／~system
］

</td></tr><tr><td>出力音量
</td><td>`volume^en
</td><td>
聴感音量（ `loudness^en ）と区別

</td></tr><tr><td>聴感音量
</td><td>`loudness^en
</td><td>
出力音量（ `volume^en ）と区別

</td></tr><tr><td>聴覚-
</td><td>`aural^en
</td><td>

</td></tr><tr><td>聴感~icon／聴感~環境
</td><td>`auditory icon^en ／ `auditory environment^en
</td><td>

</td></tr><tr><td>聴取／聴取者
</td><td>`listening^en ／ `listener^en
</td><td>

</td></tr><tr><td>~cue／`音声~cue$
</td><td>`cue^en ／ `audio cue^en
</td><td>
何かを合図する音。
“~cue” は、
この仕様の文脈においては音声~cueの略称。

</td></tr><tr><td>`~rest$
</td><td>`rest^en
</td><td>
“休止”。
`~pause$と似るが，`聴覚-~box~model$の中での位置, および連接-時の挙動が異なる

</td></tr><tr><td>`~pause$
</td><td>`pause^en
</td><td>
“静止”。

</td></tr><tr><td>韻律
</td><td>`prosodic^en
</td><td>
【`参考＠https://ja.wikipedia.org/wiki/%E9%9F%BB%E5%BE%8B_%28%E8%A8%80%E8%AA%9E%E5%AD%A6%29$】

</td></tr><tr><td>韻律~境界
</td><td>`prosodic boundary^en
</td><td>
特定の持続時間を伴う無音

</td></tr><tr><td>分断
</td><td>`break^en
</td><td>
~restや~pauseなどにより生じる間（ま）

</td></tr><tr><td>連接
</td><td>`adjoining^en
</td><td>
境界を共有する隣接

</td></tr><tr><td>音響, 音
</td><td>`sound^en
</td><td>

</td></tr><tr><td>無音
</td><td>`silence^en, `silent^en
</td><td>
音は出ないが時間は消費する

</td></tr><tr><td>半音
</td><td>`semitone^en
</td><td>

</td></tr><tr><td>音高
</td><td>`pitch^en
</td><td>

</td></tr><tr><td>音階
</td><td>`scale^en
</td><td>

</td></tr><tr><td>音素
</td><td>`phoneme^en
</td><td>

</td></tr><tr><td>発声する
</td><td>`announce^en
</td><td>

</td></tr><tr><td>発音
</td><td>`pronunciation^en
</td><td>

</td></tr><tr><td>音標
</td><td>`phonetic^en
</td><td>
発音表記に用いられる記法／文字

</td></tr><tr><td>再生する／再生（体言）
</td><td>`play^en ／ `playback^en
</td><td>

</td></tr><tr><td>自然言語
</td><td>`language^en
</td><td>
人工言語と区別

</td></tr><tr><td>相殺
</td><td>`collapsing^en
</td><td>
`視覚-~box~model$における`~margin相殺＠~CSS2J#collapsing-margins$の概念に相当する。

</td></tr></tbody></table>
<!-- 
<tr><td>振幅
<td>amplitude
<td>

<tr><td>波形
<td>waveform
<td>
-->
	</section>
	<section id="intro">
<h2 title="Introduction, design goals">1. 序論, 設計~目標</h2>

◎非規範的

<p>
聴覚-情報の呈示は、［
低視力者から／印字不能なとき
］に共通的に利用される。
例えば， “~screen-reader（画面読み取り器）” は、
視覚-~interfaceの下で，［
他の仕方では~access不能な情報をヤリトリする
］ことを利用者に許容する。
また，個人の身体的な情報~access能を問わず、
内容の （`読み取り^emとは対照的に）`聴取^emが選好されたり，ときには要求される状況下もある。
例として
⇒＃
電子本の再生-時,
車の運転時,
［工業用／医療用］機器の操作方法の習得,
在宅鑑賞~systemとのヤリトリ,
年少者~向けの読み方の指南,
等々
◎
The aural presentation of information is commonly used by people who are blind, visually-impaired or otherwise print-disabled. For instance, “screen readers” allow users to interact with visual interfaces that would otherwise be inaccessible to them. There are also circumstances in which listening to content (as opposed to reading) is preferred, or sometimes even required, irrespective of a person’s physical ability to access information. For instance: playing an e-book whilst driving a vehicle, learning how to manipulate industrial and medical devices, interacting with home entertainment systems, teaching young children how to read.
</p>

<p>
CSS Speech ~module（以下，単に “この~module” と称する）に定義される各種~CSS~propにより、
作者が［
`聴覚-次元$における文書の呈示を宣言的に制御する
］ことを可能化する。
文書の音声化は、
次の組合nである：
◎
The CSS properties defined in the Speech module enable authors to declaratively control the presentation of a document in the aural dimension. The aural rendering of a document combines＼
</p>
<ul>
	<li>
発話~合成
（ “~TTS”
— `Text to Speech^en の頭字語 —
としても知られている）
◎
speech synthesis (also known as “TTS”, the acronym for “Text to Speech”) and＼
</li>
	<li>
聴感~icon
（この仕様の中では， “音声~cue” と称される）
◎
auditory icons (which are referred-to as “audio cues” in this specification).
</li>
</ul>

<p>
CSS Speech（以下，単に “この仕様” と称する）の~propは、［
発話の音高,
発話の速度,
音の各種~level,
~TTS~voice
］その他を制御する能を供する。
これらの~propは、
視覚-~propと併用したり（混成~媒体），視覚-呈示に対する完全な聴覚-代替にもなり得る。
◎
The CSS Speech properties provide the ability to control speech pitch and rate, sound levels, TTS voices, etc. These stylesheet properties can be used together with visual properties (mixed media), or as a complete aural alternative to a visual presentation.
</p>

	</section>
	<section id="background">
<h2 title="Background information, CSS 2.1">2. 背景0情報, CSS 2.1</h2>

◎非規範的

<p>
この~moduleは、
CSS 2.1 の
`§ Aural 参考~付録＠~CSS22/aural.html#aural$
の作り直しである。
その仕様は、
`媒体~型$ `aural$v を述べていたが，非推奨にされた
（ `speech$v （発話）媒体~型への支持を受けて
— それも今や非推奨にされたが）。
`CSS2$r 仕様は，`媒体~型$ `speech^v を予約していたが、
実際には，対応する~propを定義していない。
この~moduleは、
発話~出力に適用できる~CSS~propについて述べる。
また、
特定的に`聴覚-次元$用に，新たな “~box” ~modelも定義する。
◎
The CSS Speech module is a re-work of the informative CSS2.1 Aural appendix, within which the aural media type was described, but also deprecated (in favor of the speech media type, which has now also been deprecated). Although the [CSS2] specification reserved the speech media type, it didn’t actually define the corresponding properties. The Speech module describes the CSS properties that apply to speech output, and defines a new “box” model specifically for the aural dimension.
</p>

<p>
内容~作成者は、
どの`媒体~型$用にも，~TTS合成~能力がある~UA用の~CSS~propを含めれる
— それらがイミを成すのは、
一般に［
`all$v ／ `screen$v
］用に限られることになるが。
これらの~styleは、
この~moduleを~supportしない~UAからは，単純に無視される。
◎
Content creators can include CSS properties for user agents with text to speech synthesis capabilities for any media type - though generally, they will only make sense for all and screen. These styles are simply ignored by user agents that do not support the Speech module.
</p>

	</section>
	<section id="ssml-rel">
<h2 title="Relationship with SSML">3.~SSMLとの関係性</h2>

◎非規範的

<p>
この仕様の中の一部の特能は、
~SSML
— Speech Synthesis Markup Language 1.1 `SSML$r —
に述べられる機能性に概念的に類似する。
しかしながら，~CSS~modelの特殊性から、
~SSMLの構文や意味論との互換性は，部分的な範囲に限られる。
この~moduleの各種~propの定義には、
~SSMLの類似な機能性との関係性を明確化するため，必要に応じて参考~情報が含められている。
◎
Some of the features in this specification are conceptually similar to functionality described in the Speech Synthesis Markup Language (SSML) Version 1.1 [SSML]. However, the specificities of the CSS model mean that compatibility with SSML in terms of syntax and/or semantics is only partially achievable. The definition of each property in the Speech module includes informative statements, wherever necessary, to clarify their relationship with similar functionality from SSML.
</p>

		<section id="values">
<h3 title="Value Definitions">3.1. 値~定義</h3>

<p class="trans-note">【
この節の内容は `~CSS日本語訳 共通~page＠~CSScommon#values$に移譲。
】</p>

		</section>
	</section>
	<section id="example">
<h2 title="Example">4. 例</h2>

  <div class="example">
<p>
下の例は、
【！作者が】~HTMLの見出しを［
`paul^v と称される~voiceにより, 
（ `normal^v より強い） `moderate^v 強勢を利用して，
発話~合成器に発話させる方法
］および［
各 見出しに対し~TTS音声化を開始する前に，音声~cue
（与えられた~URLに在る録音-済みな音声~clip）
を挿入する方法
］を示す。
~stereo音響~systemにおいては、
~CSS~class `heidi^css が付与された段落（
`p^e 要素
）が左~音声~channelに（および，女性~voice, その他により）音声化され，
~class `peter^css
のものが右~channelに（および，男性~voice, その他により）音声化される。
~class `special^css
が付与された `span^e 要素では、
~textの出力音量~levelが 通常より低くされた上で、
それが発話された後に強い~pauseが導入されることにより，韻律~境界が作成される（
~HTMLの `span^e はその親の段落から `voice-family$p を継承することに注意）。
◎
This example shows how authors can tell the speech synthesizer to speak HTML headings with a voice called "paul", using moderate emphasis (which is more than normal) and how to insert an audio cue (pre-recorded audio clip located at the given URL) before the start of TTS rendering for each heading. In a stereo-capable sound system, paragraphs marked with the CSS class heidi are rendered on the left audio channel (and with a female voice, etc.), whilst the class peter corresponds to the right channel (and to a male voice, etc.). The volume level of text spans marked with the class special is lower than normal, and a prosodic boundary is created by introducing a strong pause after it is spoken (note how the span inherits the voice-family from its parent paragraph).
</p>

<pre class="lang-css">
h1, h2, h3, h4, h5, h6 {
  `voice-family$p: 太郎;        /* <span class="comment">特定の~voice名</span> */
  `voice-stress$p: moderate;    /* <span class="comment">強勢する度合い</span> */
  `cue-before$p: url(../audio/ping.wav); /* <span class="comment">音声~cue</span> */
  `voice-volume$p: medium 6dB;  /* <span class="comment">聴感音量＋出力音量の補正量</span> */
}
p.花子 {
  `voice-family$p: female;      /* <span class="comment">女性~voice</span> */
  `voice-balance$p: left;       /* <span class="comment">左~channel</span> */
  `voice-pitch$p: high;         /* <span class="comment">高めの音高</span> */
  `voice-volume$p: -6dB;        /* <span class="comment">出力音量の補正量</span> */
}
p.一郎 {
  `voice-family$p: male;        /* <span class="comment">男性~voice</span> */
  `voice-balance$p: right;      /* <span class="comment">右~channel</span> */
  `voice-rate$p: fast;          /* <span class="comment">速い発話</span> */
}
span.special {
  `voice-volume$p: soft;        /* <span class="comment">控えめな聴感音量</span> */
  `pause-after$p: strong;       /* <span class="comment">強い~pause</span> */
}
</pre>
…
<pre class="lang-html">&lt;h1&gt;太郎です。見出しの読み上げを担当します。&lt;/h1&gt;
&lt;p class="花子"&gt;こんにちは、
私は花子。&lt;/p&gt;
&lt;p class="一郎"&gt;
  &lt;span class="special"&gt;聴こえますか？&lt;/span&gt;
  僕は一郎。
&lt;/p&gt;
</pre>

<!-- 
I am Paul, and I speak headings.
Hello, I am Heidi.
Can you hear me ?
  I am Peter.

 -->
</div>

	</section>
	<section id="aural-model">
<h2 title="The aural formatting model">5. 聴覚-整形~model</h2>

<p>
聴覚-媒体~用の~CSS整形~modelは、
`視覚-~box~model$に類似な，入子な文脈の中で生じる［
音と無音
］が成す連列に基づいている。
ここでは それを，
<dfn id="aural-box-model">聴覚- “~box” ~model</dfn>
と称することにする。
聴覚- “~canvas” は、
`聴覚-次元@
— ［
合成な発話と音声~cue
］が共存し得るような，［
2 個の~channelが成す（~stereo）空間, および
時間的な次元
］
— からなる。
要素は、
（内側から外側にかけて）順に［
`rest$p, `cue$p, `pause$p
］~propで囲まれる。
これらは それぞれ，聴覚-において［
`padding$p, `border$p, `margin$p
］~propに等価なものと捉えられる。
［
`before$pe ／ `after$pe
］疑似要素が利用されたときは，要素の内容と `rest$p の狭間に挿入される。
◎
The CSS formatting model for aural media is based on a sequence of sounds and silences that occur within a nested context similar to the visual box model, which we name the aural “box” model. The aural “canvas” consists of a two-channel (stereo) space and of a temporal dimension, within which synthetic speech and audio cues coexist. The selected element is surrounded by rest, cue and pause properties (from the innermost to the outermost position). These can be seen as aural equivalents to padding, border and margin, respectively. When used, the ::before and ::after pseudo-elements [CSS2] get inserted between the element’s contents and the rest.
</p>

<p>
次の図式に、
要素に適用される［
`視覚-~box~model$, `聴覚-~box~model$
］の~propの間の等価性を示す：
◎
The following diagram illustrates the equivalence between properties of the visual and aural box models, applied to the selected &lt;element&gt;:
</p>

<figure>
【！aural-box.png】
<ol id="aural-box">
	<li style="top:1em; bottom:1em; left:2em;">
`pause-before^p
<div style="height:9em;"></div>
<div style="height:9em;"></div>
`margin-top^p
</li>
	<li style="top:2.5em; bottom:2.5em; left:3em;">
`cue-before^p
<div style="height:7.5em;"></div>
<div style="height:7.5em;"></div>
`border-top^p
</li>
	<li style="top:4em; bottom:4em; left:4em;">
`rest-before^p
<div style="height:6em;"></div>
<div style="height:6em;"></div>
`padding-top^p
</li>

	<li style="top:10em; left:0; border: solid black 0.1em; width:25em; height:0;">
<div style="top:-0.5em; left: 6em; padding:0 0.5em; width:auto; background: white; border: none;">要素</div>
<div style="left: 25em; top:-0.3em; border:solid transparent 0.3em; border-left: solid black 1.5em;"><!-- 矢先 --></div>
</li>

	<li style="top:5.5em; bottom:5.5em; left:12em;">
`rest-after^p
<div style="height:4.5em;"></div>
<div style="height:4.5em;"></div>
`padding-bottom^p
</li>
	<li style="top:7em; bottom:7em; left:13em;">
`cue-after^p
<div style="height:3em;"></div>
<div style="height:3em;"></div>
`border-bottom^p
</li>
	<li style="top:8.5em; bottom:8.5em; left:14em;">
`pause-after^p
<div style="height:1.5em;"></div>
<div style="height:1.5em;"></div>
`margin-bottom^p
</li>
</ol>
<figcaption>
【！ alt caption 】
図式による聴覚- “~box” ~model：
中央に要素が位置し，その
左側には（内側から外側にかけて）［
`rest-before$p, `cue-before$p, `pause-before$p
］が並び,
右側には（内側から外側にかけて）［
`rest-after$p, `cue-after$p, `pause-after$p
］が並ぶ。
ここで、［
`rest-*^p は `padding-*^p ／
`cue-*^p は `border-*^p ／
`pause-*^p は `margin-*^p
］に，概念的に類似する。
◎
The aural 'box' model, illustrated by a diagram: the selected element is positioned in the center, on its left side are (from innermost to outermost) rest-before, cue-before, pause-before, on its right side are (from innermost to outermost) rest-after, cue-after, pause-after, where rest is conceptually similar to padding, cue is similar to border, pause is similar to margin.
</figcaption>

</figure>

	</section>
	<section id="mixing-props">
<h2 title="Mixing properties">6.~mixing~prop</h2>

		<section id="mixing-props-voice-volume">
<h3 title="The voice-volume property">6.1. `voice-volume^p ~prop</h3>

`voice-volume^vv

◎名 `voice-volume@p
◎値
`silent$v
| [[`x-soft$v
| `soft$v
| `medium$v
| `loud$v
| `x-loud$v]
|| `decibel$vt]
◎初 `medium$v
◎適 すべての要素
◎継 される
◎百 受容しない
◎算
`silent$v, または［
それ以外の~keyword値と, 非 0 の~decibel~offset（~option）
］の組
◎
silent, or a keyword value and optionally also a decibel offset (if not zero)
◎順 文法に従う
◎表終

<p>
`voice-volume$p ~propは、
発話~合成器から生成される音声~波形の振幅を，作者が制御できるようにする。
また、
`聴覚-~box~model$の中での，要素の`音声~cue$の相対的な出力音量~levelの調整-時にも利用される。
◎
The voice-volume property allows authors to control the amplitude of the audio waveform generated by the speech synthesizer, and is also used to adjust the relative volume level of audio cues within the aural box model of the selected element.
</p>

<p class="note">注記：
この~propが供する機能性は，
`~SSML$の `prosody$sS 要素の `volume^a 属性に類似するが、
留意すべき不一致がある。
例えば，この~propに対する［
~keyword ／ `decibel$t
］値は、
要素に継承された値と組合せる方法があることに因り，排他的でない。
◎
Note: Although the functionality provided by this property is similar to the volume attribute of the prosody element from the SSML markup language [SSML], there are notable discrepancies. For example, CSS Speech volume keywords and decibels units are not mutually-exclusive, due to how values are inherited and combined for selected elements.
</p>

<dl class="valdef">
	<dt>`silent@v</dt>
	<dd>
（大きさのある）音は生成されない（~textは “無音に” 読まれる）ことを指定する。
◎
Specifies that no sound is generated (the text is read "silently").
</dd>
	<dd class="note">注記：
これは、
負な無限大~decibelの利用と同じ効果になることに注意。
また、
`voice-volume$p ~propの値が
`silent$v にされた要素と,
`speak$p ~propの値が
`none^v にされた要素との間には，相違があることにも注意。
前者では、
要素は，前後の~pauseも含め，それが発話されたかのように同じ時間を占めつつ, 大きさのある音は生成されない
（ただし，要素の`聴覚-~box~model$の中の子孫は、
`voice-volume$p 値を上書きできるので，音声~出力を生成し得る）。
一方で後者では、
要素は`聴覚-次元$の中に音声化されず，再生~用の時間も割振られていない
（ただし，要素の`聴覚-~box~model$の中の子孫は、
`speak$p 値を上書きできるので，音声~出力を生成し得る）。
◎
Note: This has the same effect as using negative infinity decibels. Also note that there is a difference between an element whose voice-volume property has a value of silent, and an element whose speak property has the value none. With the former, the selected element takes up the same time as if it was spoken, including any pause before and after the element, but no sound is generated (and descendants within the aural box model of the selected element can override the voice-volume value, and may therefore generate audio output). With the latter, the selected element is not rendered in the aural dimension and no time is allocated for playback (descendants within the aural box model of the selected element can override the speak value, and may therefore generate audio output).
</dd>

	<dt>`x-soft@v</dt>
	<dt>`soft@v</dt>
	<dt>`medium@v</dt>
	<dt>`loud@v</dt>
	<dt>`x-loud@v</dt>
	<dd>
これら各~keywordは、［
知覚される聴感音量（ `loudness^en ）に関する聴取者の要件を満たす出力音量~level
］に対応する値に対応付けられ，実装に依存する。
後に挙げたものほど，大きくなる（厳密には，より小さくない）。
これらの音声~levelは、
概して，選好の仕組み
— ［
各自の聴感~環境に則って，音についての~optionを較正する
］ことを利用者に許容する仕組み —
を介して供される。
◎
This sequence of keywords corresponds to monotonically non-decreasing volume levels, mapped to implementation-dependent values that meet the listener’s requirements with regards to perceived loudness. These audio levels are typically provided via a preference mechanism that allow users to calibrate sound options according to their auditory environment.＼
</dd>
	<dd>
これら各~keywordは、
次に対応付けられる
⇒＃
`x-soft^v は利用者が`聴取れる最小^emな出力音量~level／
`x-loud^v は利用者が`容認し得る最大^emな出力音量~level／
`medium^v は利用者が `選好する^em出力音量~level／
`soft^v は `x-soft^v と `medium^v の中間の値／
`loud^v は `x-loud^v と `medium^v の中間の値
◎
The keyword x-soft maps to the user’s minimum audible volume level, x-loud maps to the user’s maximum tolerable volume level, medium maps to the user’s preferred volume level, soft and loud map to intermediary values.
</dd>

	<dt id="valdef-voice-volume-decibel">`decibel$t</dt>
	<dd>
<p>
次に挙げるいずれかの値に相対的な（正な／負な）変化を表現する：
◎
This represents a change (positive or negative) relative ＼
</p>

		<ol>
			<li>
（上に列挙した）~keyword値も与えられた場合、
それに対応する値。
◎
the given keyword value (see enumeration above),＼
</li>
			<li>
他の場合，`根~要素$に対しては既定~値
【すなわち，初期~値】。
◎
or to the default value for the root element,＼
</li>
			<li>
<p>
他の場合，継承された出力音量~level
（それ自身，~keyword値と~decibel~offsetの組合nをとり得る
— この場合の~decibel値は加法的に組合され得る）。
◎
or otherwise to the inherited volume level (which may itself be a combination of a keyword value and decibel offset, in which case the decibel values are combined additively).＼
</p>

<p>
この場合，継承された出力音量~levelが `silent$v のときは、
この `voice-volume$p も
— 指定された `decibel$t 値を問わず —
`silent$v に解決される。
◎
When the inherited volume level is silent, this voice-volume resolves to silent too, regardless of the specified &lt;decibel&gt; value.
</p>
			</li>
		</ol>
	</dd>
	<dd>
<p>
`decibel@t
型は、
単位~識別子 `dB^u （~decibel単位）を伴う`次元$を表す。
~decibelは、［
現在の振幅 %a0 に対する新たな信号~振幅 %a1 の比率
］の 2 乗を表現する
— 次の対数-等式に従うような
⇒
出力音量（ dB ） = 20 ~MUL log<sub>10</sub> ( %a1 ~DIV %a0 )
◎
The &lt;decibel&gt; type denotes a dimension with a "dB" (decibel unit) unit identifier. Decibels represent the ratio of the squares of the new signal amplitude a1 and the current amplitude a0, as per the following logarithmic equation: volume(dB) = 20 × log10(a1 / a0).
</p>

<p class="note">注記：
［
−6.0dB ／ +6.0dB
］は、
音声~信号の振幅において，およそ［
半分／ 2 倍
］になる。
◎
Note: -6.0dB is approximately half the amplitude of the audio signal, and +6.0dB is approximately twice the amplitude.
</p>
	</dd>
</dl>

<p class="note">注記：
知覚される聴感音量は、
様々な要因
— 利用者の［
聴取~環境, 選好, 身体能力
］など —
に依存することに注意。
`x-soft$v から `x-loud$v
までの有効な出力音量の変動範囲は、
音声~出力の（聴感音量~上の）~dynamic-rangeを表現する。
この範囲は概して，騒がしい環境下では圧縮されることになろう
— すなわち、
`x-soft$v に対応する 知覚される聴感音量は，静かな環境下のときより実質的に `x-loud$v に近付くことになろう。
また、
思慮分別が~~求められる聴取~環境（例：図書館や夜中の読書）など，［
`x-soft$v, `x-loud$v
］どちらも低い出力音量~levelに対応付けられる状況もあろう。
◎
Note: Perceived loudness depends on various factors, such as the listening environment, user preferences or physical abilities. The effective volume variation between x-soft and x-loud represents the dynamic range (in terms of loudness) of the audio output. Typically, this range would be compressed in a noisy context, i.e. the perceived loudness corresponding to x-soft would effectively be closer to x-loud than it would be in a quiet environment. There may also be situations where both x-soft and x-loud would map to low volume levels, such as in listening environments requiring discretion (e.g. library, night-reading).
</p>

		</section>
		<section id="mixing-props-voice-balance">
<h3 title="The voice-balance property">6.2. `voice-balance^p ~prop</h3>

`voice-balance^vv

◎名 `voice-balance@p
◎値
`number$vt
| `left$v
| `center$v
| `right$v
| `leftwards$v
| `rightwards$v
◎初 `center$v
◎適 すべての要素
◎継 される
◎百 受容しない
◎算
−100 以上 100 以下の `number$t に解決される指定d値。
◎
the specified value resolved to a &lt;number&gt; between -100 and 100 (inclusive)
◎順 文法に従う
◎表終

<p>
`voice-balance$p ~propは、
音響~stageにおける［
側面方向
— 聴取者から見て，左手側にある端から右手側にある端まで —
にまたがる，音声~出力の空間的な分布
］を制御する。
作者は、
左右端の中間~段階を指定して，結果における［
左右軸に沿った音声~分離
］を表現できる。
◎
The voice-balance property controls the spatial distribution of audio output across a lateral sound stage: one extremity is on the left, the other extremity is on the right hand side, relative to the listener’s position. Authors can specify intermediary steps between left hand right extremities, to represent the audio separation along the resulting left-right axis.
</p>

<p class="note">注記：
`~SSML$には、
この~propが供する機能性に合致するものは無い。
◎
Note: The functionality provided by this property has no match in the SSML markup language [SSML].
</p>

<dl class="valdef">
	<dt id="valdef-voice-balance-number">`number$t</dt>
	<dd>
−100 以上 100 以下の実数。
範囲~外の値は範囲~内に切詰められる。
負な数は左側を表現し, 正な数は右側を表現する。
`0^v は、
左右の音声~分離を聴き分け得ない，中央~点を表現する
（~stereo音響~systemの下では、
これは，左右~speakerへの音声~信号の分布が等しいことに対応する）。
◎
A number between -100 and 100 (inclusive). Values smaller than -100 are clamped to -100. Values greater than 100 are clamped to 100. The value -100 represents the left side, and the value 100 represents the right side. The value 0 represents the center point whereby there is no discernible audio separation between left and right sides. (In a stereo sound system, this corresponds to equal distribution of audio signals between left and right speakers).
</dd>

	<dt>`left@v</dt>
	<dd>
`-100^v と同じ。
◎
Same as -100.
</dd>

	<dt>`center@v</dt>
	<dd>
`0^v と同じ。
◎
Same as 0.
</dd>

	<dt>`right@v</dt>
	<dd>
`100^v と同じ。
◎
Same as 100.
</dd>

	<dt>`leftwards@v</dt>
	<dd>
音を左へ移動させる
— `voice-balance$p の継承d値から 20 を減算する（および −100 以上に切上げる）ことにより。
◎
Moves the sound to the left by subtracting 20 from the inherited voice-balance value (and by clamping the resulting number to -100).
</dd>

	<dt>`rightwards@v</dt>
	<dd>
音を右へ移動させる
— `voice-balance$p の継承d値に 20 を加算する（および 100 以下に切下げる）ことにより。
◎
Moves the sound to the right, by adding 20 to the inherited voice-balance value (and by clamping the resulting number to 100).
</dd>
</dl>

<p>
~UAに接続される音響~systemに備わる音声~mixingには、
その特能として様々な能力があり得る。
［
~mono, ~stereo, ~surround
］音響~systemに期待される挙動は、
次に従って定義される：
◎
User agents can be connected to different kinds of sound systems, featuring varying audio mixing capabilities. The expected behavior for mono, stereo, and surround sound systems is defined as follows:
</p>
<ul>
	<li>
~UAが~mono-aural（すなわち，単独の~speakerからなる）音響~systemを通して音声を生産する下では、
`voice-balance$p ~propの効果は無い。
◎
When user agents produce audio via a mono-aural sound system (i.e. single-speaker setup), the voice-balance property has no effect.
</li>
	<li>
~UAが~stereo音響~system（例：~headphoneや 2 個の~speaker）を通して音声を生産する下では、
音声~信号の左右間の分布は，
`voice-balance$p ~prop用に著作された値に精確に合致し得る。
◎
When user agents produce audio through a stereo sound system (e.g. two speakers, or a pair of headphones), the left-right distribution of audio signals can precisely match the authored values for the voice-balance property.
</li>
	<li>
~UAが多~channel
（例：専用の中央~channelも備える 5 個の~speakerからなる~surround音響~system）
を通した音声~信号の~mixingも可能な下では、
`voice-balance$p ~propの適用による結果の 音声~信号の物理的な分布は、
聴取者からは，基本的な~stereo~layoutから音が鳴らされているかのように知覚されるべきである。
例えば、
`center$v 値の挙動を模倣するために，中央~channelも左右~speakerと併用されてもヨイ。
◎
When user agents are capable of mixing audio signals through more than 2 channels (e.g. 5-speakers surround sound system, including a dedicated center channel), the physical distribution of audio signals resulting from the application of the voice-balance property should be performed so that the listener perceives sound as if it was coming from a basic stereo layout. For example, the center channel as well as the left/right speakers may be used all together in order to emulate the behavior of the center value.
</li>
</ul>

<p>
この~moduleの将来の改訂には、
作者が実質的に “方位角” と “仰角” の値を指定できるような，
三次元な音声~用の~supportも含められ得る。
したがって，現在の仕様を利用して著作された内容は、将来には［
この仕様の~versionのうち，三次元な音声を~supportするもの
］に準拠する~UAからも消費され得る。
この可能性に備えるため、
現在の `voice-balance$p ~propに可能化されている値は，
“方位角” による角度との互換性が維持されるように設計されている。
より精確には、［
現在の左右間の音声~軸（側面方向の音響~stage）
］と［
聴取者から見た周囲 360 度 平面
］との対応付けが，次に従って定義される：
◎
Future revisions of the CSS Speech module may include support for three-dimensional audio, which would effectively enable authors to specify “azimuth” and “elevation” values. In the future, content authored using the current specification may therefore be consumed by user agents which are compliant with the version of CSS Speech that supports three-dimensional audio. In order to prepare for this possibility, the values enabled by the current voice-balance property are designed to remain compatible with “azimuth” angles. More precisely, the mapping between the current left-right audio axis (lateral sound stage) and the envisioned 360 degrees plane around the listener’s position is defined as follows:
</p>

<ul>
	<li>
値 `0^v は、
0 度（ `center$v ）に対応付けられる。
これは、
聴取者の  “背後から” ではなく， “正面から” になる。
◎
The value 0 maps to zero degrees (center). This is in "front" of the listener, not from "behind".
</li>
	<li>
値 `-100^v は， −40 度（ `left$v ）に対応付けられる。
負な角度は，（音声~stageを真上から眺めて）反時計回りの方向を表す。
◎
The value -100 maps to -40 degrees (left). Negative angles are in the counter-clockwise direction (the audio stage is seen from the top).
</li>
	<li>
値 `100^v は， 40 度（ `right$v ）に対応付けられる。
正な角度は，（音声~stageを真上から眺めて）時計回りの方向を表す。
◎
The value 100 maps to 40 degrees (right). Positive angles are in the clockwise direction (the audio stage is seen from the top).
</li>
	<li>
−100 以上 100 以下の尺度による値は、
数値的に線形に比例するように，
−40 度~以上 40 度~以下の角度に対応付けられる。
例えば， −50 は −20 度に対応付けられる。
◎
Intermediary values on the scale from 100 to 100 map to the angles between -40 and 40 degrees in a numerically linearly-proportional manner. For example, -50 maps to -20 degrees.
</li>
</ul>

<p class="note">注記：
利用者は 音響~systemを環境設定することもあるので、
文書~作者により指定される左右間の音声~分布は，それに干渉され得ることに注意。
現代の音響~systemは，概して、
様々な “~surround” ~modeを備えており
（基本的な~stereo~speaker~systemも含む）、
知覚される音声~信号の空間的な~layoutが大きく改められる傾向にある。
三次元な音響~stageによる錯覚効果は、［
位相変位, ~digital遅延, 出力音量~制御（~channel~mixing）, その他の技法
］の組合nを利用して達成されることが多い。
一部の利用者は、
自身の~systemを［
音声化される音を単独の~mono~channelに “~downgrade” する
］ことすらある
— この場合、
`voice-balance$p ~propの効果は，明白に知覚し得なくなる。
したがって，著作された内容の音声化における忠実さは、
利用者による そのような~custom化に依存する。
`voice-balance$p ~propは、
単に欲される最終結果を指定するものに過ぎない。
◎
Note: Sound systems can be configured by users in such a way that it would interfere with the left-right audio distribution specified by document authors. Typically, the various “surround” modes available in modern sound systems (including systems based on basic stereo speakers) tend to greatly alter the perceived spatial arrangement of audio signals. The illusion of a three-dimensional sound stage is often achieved using a combination of phase shifting, digital delay, volume control (channel mixing), and other techniques. Some users may even configure their system to “downgrade” any rendered sound to a single mono channel, in which case the effect of the voice-balance property would obviously not be perceivable at all. The rendering fidelity of authored content is therefore dependent on such user customizations, and the voice-balance property merely specifies the desired end-result.
</p>

<p class="note">注記：
多くの発話~合成器は、
~mono音しか生成しないので，
`voice-balance$p ~propを内在的に~supportしないことに注意。
そのため、
左右軸に沿う音の分布は，合成の後処理~段
（発話が可能化された~UAが，文書~内に著作された様々な音声~sourceを混合する段）
で生じる。
◎
Note: Many speech synthesizers only generate mono sound, and therefore do not intrinsically support the voice-balance property. The sound distribution along the left-right axis consequently occurs at post-synthesis stage (when the speech-enabled user agent mixes the various audio sources authored within the document)
</p>

		</section>
	</section>
	<section id="speaking-props">
<h2 title="Speaking properties">7. 発話用~prop</h2>

		<section id="speaking-props-speak">
<h3 title="The speak property">7.1. `speak^p ~prop</h3>

`speak^vv

◎名 `speak@p
◎値
`auto$v
| `never$v
| `always$v
◎初 `auto$v
◎適 すべての要素
◎継 される
◎百 受容しない
◎算 指定されたとおり
◎順 文法に従う
◎表終

<p>
`speak$p ~propは、
~textを音声化するかどうかを決定する。
◎
The speak property determines whether or not to render text aurally.
</p>

<p class="note">注記：
`~SSML$には、
この~propが供する機能性に合致するものは無い。
◎
Note: The functionality provided by this property has no match in the SSML markup language [SSML].
</p>

<dl class="valdef">
	<dt>`auto@v</dt>
	<dd>
算出d値は、
`display$p に応じて［
`none^v ならば `never$v ／
~ELSE_ `auto$v
］に解決される。
算出d値 `auto$v の使用~値は、
`visibility$p に応じて［
`visible^v ならば `always$v ／
~ELSE_ `never$v
］に等価になる。
◎
Resolves to a computed value of never when display is none, otherwise resolves to a computed value of auto. The used value of a computed auto is equivalent to always if visibility is visible and to never otherwise.
</dd>
	<dd class="note">注記：
`display$p ~propの `none^v 値は，要素の子孫からは上書きされ得ない一方で、
`speak$p の `auto$v 値は， `never$v や `always$v 利用して上書きできることに注意。
◎
Note: The none value of the display property cannot be overridden by descendants of the selected element, but the auto value of speak can however be overridden using either of never or always.
</dd>

	<dt>`never@v</dt>
	<dd>
この値は、
（~pause, ~cue, ~rest, 実際の内容も含め）
要素が音声化されないようにする
（すなわち，要素は`聴覚-次元$の下では効果を持たなくなる）。
◎
This value causes an element (including pauses, cues, rests and actual content) to not be rendered (i.e., the element has no effect in the aural dimension).
</dd>
	<dd class="note">注記：
影響される要素の子孫では，この値の上書きが許容されるので、
この~levelで `display$p に `none^v を利用していたとしても，子孫は実際に音声化の一部を成し得ることに注意。
しかしながら、
先祖~要素の［
~pause, ~cue, ~rest
］は，`聴覚-次元$の下では “作動中でない” ままにされるため、
`~pauseの相殺$や［
連接している~restにおける加法的な挙動
］には，寄与しない。
◎
Note: Any of the descendants of the affected element are allowed to override this value, so descendants can actually take part in the aural rendering despite using display: none at this level. However, the pauses, cues, and rests of the ancestor element remain “deactivated” in the aural dimension, and therefore do not contribute to the collapsing of pauses or additive behavior of adjoining rests.
</dd>

	<dt>`always@v</dt>
	<dd>
要素は音声化される
（要素の `display$p 値や先祖の［
`display$p ／ `speak$p
］値に関わらず）。
◎
The element is rendered aurally (regardless of its display value, or the display or speak values of its ancestors).
</dd>
	<dd class="note">注記：
この値の利用により、
要素は，視覚的な~canvasには描画されなくても，`聴覚-次元$の下では音声化されるようになることに注意。
◎
Note: Using this value can result in the element being rendered in the aural dimension even though it would not be rendered on the visual canvas.
</dd>
</dl>

		</section>
		<section id="speaking-props-speak-as">
<h3 title="The speak-as property">7.2. `speak-as^p ~prop</h3>

`speak-as^vv

◎名 `speak-as@p
◎値
`normal$v
| `spell-out$v
|| `digits$v
|| [ `literal-punctuation$v
| `no-punctuation$v ]
◎初 `normal$v
◎適 すべての要素
◎継 される
◎百 受容しない
◎算 指定されたとおり
◎順 文法に従う
◎表終

<p>
`speak-as$p ~propは、
定義済みな~~選択肢に基づいて，~textがどの方式で音声化されるかを決定する。
◎
The speak-as property determines in what manner text gets rendered aurally, based upon a predefined list of possibilities.
</p>

<p class="note">注記：
この~propが供する機能性は、
`~SSML$の `say-as$sS 要素に概念的に類似することに注意
（アリな値は `SSML-SAYAS$r にて述べられている）。
設計~目標は類似するが、
~CSS~modelでは，発音~規則の基本的な集合に制限される。
◎
Note: The functionality provided by this property is conceptually similar to the say-as element from the SSML markup language [SSML] (whose possible values are described in the [SSML-SAYAS] W3C Note). Although the design goals are similar, the CSS model is limited to a basic set of pronunciation rules.
</p>

<dl class="valdef">
	<dt>`normal@v</dt>
	<dd>
要素の内容の音声化に，自然言語に依存する発音~規則を利用する。
例えば約物は、
そのまま発話されることなく，
代わりに適切な~pauseとして “自然に” 音声化される。
◎
Uses language-dependent pronunciation rules for rendering the element’s content. For example, punctuation is not spoken as-is, but instead rendered naturally as appropriate pauses.
</dd>

	<dt>`spell-out@v</dt>
	<dd>
~textを一字ごとに読み綴る
（頭字語や略語に有用になる）。
~accent付き文字が稀な自然言語の下では、
~accentなしの綴りによる代替も許可される。
例として，~Englishでは単語 "rôle" を "`role^en" とも記せるので、
適合~実装は， "rôle" を “R O L E” と読み綴ることになる。
◎
Spells the text one letter at a time (useful for acronyms and abbreviations). In languages where accented characters are rare, it is permitted to drop accents in favor of alternative unaccented spellings. As an example, in English, the word “rôle” can also be written as “role”. A conforming implementation would thus be able to spell-out “rôle” as “R O L E”.
</dd>

	<dt>`digits@v</dt>
	<dd>
数を，各~数字ごとに発話する。
一例として、［
"`twelve^en" は “`one two^en” ／
"31" は “`three one^en”
］と発話されることになろう。
◎
Speak numbers one digit at a time, for instance, “twelve” would be spoken as “one two”, and “31” as “three one”.
</dd>
	<dd class="trans-note">【
おそらく，要素の`内容~言語$が~Englishである下では。
"`twelve^en" については違和感も
— そのような発話が想定されているなら，通常は 10 進~記数法で記されるだろうし、
そのような発話を避けるべく "`twelve^en" と記されることもあろう。
日本語の下では，［
"十二" は “いち, に” ／
"31" は “さん, いち”
］と発話されることになろうが、
"twelve" も “いち, に” と発話されるべきかどうかは，よくわからない。
】</dd>
	<dd class="note">注記：
発話~合成器は`数^emについての知識を持ち得る。
`speak-as$p ~propは、［
~UAが数をどう音声化するか
］に対する，ある程度の制御を可能化する
— それは、［
~textを実際の発話~合成器に渡す前の前処理~段
］として実装され得る。
◎
Speech synthesizers are knowledgeable about what a number is. The speak-as property enables some level of control on how user agents render numbers, and may be implemented as a preprocessing step before passing the text to the actual speech synthesizer.
</dd>

	<dt>`literal-punctuation@v</dt>
	<dd>
~semicolon, 括弧類, 等々の約物は、
適切な~pauseとして “自然に” 音声化される代わりに，その名前が~~声に出される
（すなわち、
逐語的に発話される）。
◎
Punctuation such as semicolons, braces, and so on is named aloud (i.e. spoken literally) rather than rendered naturally as appropriate pauses.
</dd>

	<dt>`no-punctuation@v</dt>
	<dd>
約物は音声化されない：
発話されず，また~pauseとしても音声化されない。
◎
Punctuation is not rendered: neither spoken nor rendered as pauses.
</dd>
</dl>

		</section>
	</section>
	<section id="pause-props">
<h2 title="Pause properties">8. ~pause~prop</h2>

		<section id="pause-props-pause-before-after">
<h3 title="The pause-before and pause-after properties">8.1. `pause-before^p, `pause-after^p ~prop</h3>

`pause-before^vv

◎名 `pause-before@p, `pause-after@p
◎値
`time [0s,∞]$vt
| `none$v
| `x-weak$v
| `weak$v
| `medium$v
| `strong$v
| `x-strong$v
◎初 `none$v
◎適 すべての要素
◎継 されない
◎百 受容しない
◎算 指定されたとおり
◎順 文法に従う
◎表終

<p>
［
`pause-before$p ／ `pause-after$p
］~propは、［
`聴覚-~box~model$の中で，要素の発話~合成による音声化nの［
前／後
］に生じる
］ような韻律~境界（特定の持続時間を伴う無音）を指定する
— これは、［
`cue-before$p ／ `cue-after$p
］が指定されている場合には，それによる~cueより［
前／後
］に生じるとする。
◎
The pause-before and pause-after properties specify a prosodic boundary (silence with a specific duration) that occurs before (or after) the speech synthesis rendition of the element, or if any cue-before (or cue-after) is specified, before (or after) the cue within the aural box model.
</p>

<p class="note">注記：
この~propが供する機能性は，
`~SSML$の `break$sS 要素に類似するが、
`聴覚-~box~model$の中での `pause$p 韻律~境界の適用には，特別な考慮が要求される
（例：`~pauseの相殺$）。
◎
Note: Although the functionality provided by this property is similar to the break element from the SSML markup language [SSML], the application of pause prosodic boundaries within the aural box model of CSS Speech requires special considerations (e.g. "collapsed" pauses).
</p>

<dl class="valdef">
	<dt id="valdef-pause-before-time">`time [0s,∞]$t</dt>
	<dd>
~pauseを絶対的な時間~単位
（秒や~mmlli秒
— 例： `+3s^v, `250ms^v ）
で表出する。
負な値は許容されない。
◎
Expresses the pause in absolute time units (seconds and milliseconds, e.g. "+3s", "250ms"). Only non-negative values are allowed.
</dd>

	<dt>`none@v</dt>
	<dd>
`0ms^v に等価
（発話~処理器からは韻律~分断は生産されない）。
◎
Equivalent to 0ms (no prosodic break is produced by the speech processor).
</dd>

	<dt>`x-weak@v</dt>
	<dt>`weak@v</dt>
	<dt>`medium@v</dt>
	<dt>`strong@v</dt>
	<dt>`x-strong@v</dt>
	<dd>
要素の狭間の~pauseを発話~出力における韻律~分断の強度で表出する。
正確な時間は実装に依存する。
後に挙げたものほど，強くなる
（厳密には，より弱くない）
【！non-decreasing (conceptually increasing) 】。
◎
Expresses the pause by the strength of the prosodic break in speech output. The exact time is implementation-dependent. The values indicate monotonically non-decreasing (conceptually increasing) break strength between elements.
</dd>
</dl>

<p class="note">注記：
より強い内容~境界には，概して~pauseも付随する。
例えば，段落と段落の間の分断は、
概して，文の中の単語と単語の間の分断よりも ずっと重みがある。
◎
Note: Stronger content boundaries are typically accompanied by pauses. For example, the breaks between paragraphs are typically much more substantial than the breaks between words within a sentence.
</p>

<div class="example">
<p>
次の例に、［
特定の要素に対する 韻律~分断の（~UA~stylesheetにて定義される）既定の強度
］を著作された~styleにより上書きする方法を示す：
◎
This example illustrates how the default strengths of prosodic breaks for specific elements (which are defined by the user agent stylesheet) can be overridden by authored styles.
</p>

<pre class="lang-css">
p { pause: none } /* <span class="comment">
`pause-before: none; pause-after: none^css
</span> */
</pre>
</div>

		</section>
		<section id="pause-props-pause">
<h3 title="The pause shorthand property">8.2. `pause^p 略式~prop</h3>

◎名 `pause@p
◎値
`pause-before$tp `pause-after$tp?
◎初 個々の~propを見よ
◎適 すべての要素
◎継 されない
◎百 受容しない
◎算 個々の~propを見よ
◎順 文法に従う
◎表終

<p>
`pause$p ~propは、［
`pause-before$p, `pause-after$p
］用の略式~propである。
2 個の値は、
順に，これらの~propの値を与える。
値が 1 個だけ与えられた場合、
それが 2 個目の値も与える。
◎
The pause property is a shorthand property for pause-before and pause-after. If two values are given, the first value is pause-before and the second is pause-after. If only one value is given, it applies to both properties.
</p>

<div class="example">
<p>
~prop値の例：
◎
Examples of property values:
</p>

<pre class="lang-css">
h1 { pause: 20ms; } /* pause-before: 20ms; pause-after: 20ms */
h2 { pause: 30ms 40ms; } /* pause-before: 30ms; pause-after: 40ms */
h3 { pause-after: 10ms; } /* pause-before: <i>未指定</i>; pause-after: 10ms */
</pre>
</div>

		</section>
		<section id="collapsed-pauses">
<h3 title="Collapsing pauses">8.3. ~pauseの相殺</h3>

<p>
この節に現れる “~box” は、
すべて，`聴覚-~box~model$の中のそれを表すとする。
</p>

<p>
~pauseは、
~boxから その［
前／後
］にある~boxまでの，最小な距離を定義する。
連接している~pauseは、［
最も強い~keywordによる分断, 最も長い絶対的な時区間
］を選定することにより，併合される。
例えば、［
`strong$v と `weak$v
］の併合-時には `strong$v が選択され，［
`1s^v と `250ms^v
］の併合-時には `1s^v が選択され，［
`strong$v と `250ms^v
］の併合-時には それらの効果が合算される。
◎
The pause defines the minimum distance of the aural "box" to the aural "boxes" before and after it. Adjoining pauses are merged by selecting the strongest named break and the longest absolute time interval. For example, "strong" is selected when merging "strong" and "weak", "1s" is selected when merging "1s" and "250ms", and "strong" and "250ms" take effect additively when merging "strong" and "250ms".
</p>

<p>
次の各 項にて挙げる 2 つの~pauseは、
連接しているとされる：
◎
The following pauses are adjoining:
</p>
<ul>
	<li>
~boxの `pause-after$p と
~boxの最後の子の `pause-after$p
— ただし、
~boxが［
`rest-after$p, `cue-after$p
］どちらも持たない【 `none^v でない（以下同様）】場合に限る。
◎
The pause-after of an aural "box" and the pause-after of its last child, provided the former has no rest-after and no cue-after.
</li>
	<li>
~boxの `pause-before$p と
~boxの最初の子の `rest-before$p
— ただし，~boxが［
`rest-before$p, `cue-before$p
］どちらも持たない場合に限る。
◎
The pause-before of an aural "box" and the pause-before of its first child, provided the former has no rest-before and no cue-before.
</li>
	<li>
~boxの `pause-after$p と
~boxの次の同胞の `pause-before$p
◎
The pause-after of an aural "box" and the pause-before of its next sibling.
</li>
	<li>
~boxの `pause-before$p と
~boxの `pause-after$p
— ただし，［
~box内に音声化される内容がまったく無い場合（ `speak$p を見よ）
］または［
~boxが ~AND↓ を満たしている場合
］に限る
⇒＃
`voice-duration$p は `0ms^v,
`rest-before$p を持たない,
`rest-after$p を持たない,
`cue-before$p を持たない,
`cue-after$p を持たない
◎
The pause-before and pause-after of an aural "box", if the "box" has a voice-duration of "0ms" and no rest-before or rest-after and no cue-before or cue-after, or if the "box" has no rendered content at all (see speak).
</li>
</ul>

<p>
相殺された【併合された】~pauseは，そのいずれかの~pause成分が別の~pauseに連接している場合、
その別の~pauseにも連接しているものと見なされる。
◎
A collapsed pause is considered adjoining to another pause if any of its component pauses is adjoining to that pause.
</p>

<p class="note">注記：
`pause$p は、
要素の内容と `cue$p との狭間から， `cue$p の外側へ移動された。
これは、
`CSS2$r § Aural 付録とは後方-互換でない。
◎
Note: Pause has been moved from between the element’s contents and any cue to outside the cue. This is not backwards compatible with the informative CSS2.1 Aural appendix [CSS2].
</p>

		</section>
	</section>
	<section id="rest-props">
<h2 title="Rest properties">9. ~rest~prop</h2>

		<section id="rest-props-rest-before-after">
<h3 title="The rest-before and rest-after properties">9.1. `rest-before^p, `rest-after^p ~prop</h3>

`rest-before^vv

◎名 `rest-before@p, `rest-after@p
◎値
`time [0s,∞]$vt
| `none$v
| `x-weak$v
| `weak$v
| `medium$v
| `strong$v
| `x-strong$v
◎初 `none$v
◎適 すべての要素
◎継 されない
◎百 受容しない
◎算 指定されたとおり
◎順 文法に従う
◎表終

<p>
［
`rest-before$p ／ `rest-after$p
］~propは、［
`聴覚-~box~model$の中で，要素の発話~合成による音声化nの［
前／後
］に生じる
］ような韻律~境界（特定の持続時間を伴う無音）を指定する。
◎
The rest-before and rest-after properties specify a prosodic boundary (silence with a specific duration) that occurs before (or after) the speech synthesis rendition of an element within the aural box model.
</p>

<p class="note">注記：
この~propが供する機能性は，
`~SSML$の `break$sS 要素に類似するが、
`聴覚-~box~model$の中での `rest$p 韻律~境界の適用には，特別な考慮が要求される
（例：点在する音声~cue, 隣接な~restの加法的な挙動）。
◎
Note: Although the functionality provided by this property is similar to the break element from the SSML markup language [SSML], the application of rest prosodic boundaries within the aural box model of CSS Speech requires special considerations (e.g. interspersed audio cues, additive adjacent rests).
</p>

<dl class="valdef">
	<dt id="valdef-rest-before-time">`time [0s,∞]$t</dt>
	<dd>
~restを絶対的な時間~単位
（秒や~mmlli秒
— 例： `+3s^v, `250ms^v ）
で表出する。
負な値は許容されない。
◎
Expresses the rest in absolute time units (seconds and milliseconds, e.g. "+3s", "250ms"). Only non-negative values are allowed.
</dd>

	<dt>`none@v</dt>
	<dd>
`0ms^v に等価
（発話~処理器からは韻律~分断は生産されない）。
◎
Equivalent to 0ms. (No prosodic break is produced by the speech processor.)
</dd>

	<dt>`x-weak@v</dt>
	<dt>`weak@v</dt>
	<dt>`medium@v</dt>
	<dt>`strong@v</dt>
	<dt>`x-strong@v</dt>
	<dd>
~restを発話~出力における 韻律~分断の強度で表出する。
正確な時間は実装に依存する。
後に挙げたものほど，強くなる
（厳密には，より弱くない）
【！non-decreasing (conceptually increasing) 】。
◎
Expresses the rest by the strength of the prosodic break in speech output. The exact time is implementation-dependent. The values indicate monotonically non-decreasing (conceptually increasing) break strength between elements.
</dd>
</dl>

<p>
`~pause~prop＠#pause-props$とは対照的に，
~restは要素の内容と［
`cue-before$p ／ `cue-after$p
］の内容との狭間に挿入される。
連接している~restは加法的に扱われ，相殺されない。
◎
As opposed to pause properties, the rest is inserted between the element’s content and any cue-before or cue-after content. Adjoining rests are treated additively, and do not collapse.
</p>

		</section>
		<section id="rest-props-rest">
<h3 title="The rest shorthand property">9.2. `rest^p 略式~prop</h3>

◎名 `rest@p
◎値
`rest-before$tp `rest-after$tp?
◎初 個々の~propを見よ
◎適 すべての要素
◎継 されない
◎百 受容しない
◎算 個々の~propを見よ
◎順 文法に従う
◎表終

<p>
`rest$p ~propは、［
`rest-before$p, `rest-after$p
］用の略式~propである。
2 個の値は、
順に，これらの~propの値を与える。
値が 1 個だけ与えられた場合、
それが 2 個目の値も与える。
◎
The rest property is a shorthand for rest-before and rest-after. If two values are given, the first value is rest-before and the second is rest-after. If only one value is given, it applies to both properties.
</p>

		</section>
	</section>
	<section id="cue-props">
<h2 title="Cue properties">10. ~cue~prop</h2>

		<section id="cue-props-cue-before-after">
<h3 title="The cue-before and cue-after properties">10.1. `cue-before^p, `cue-after^p ~prop</h3>

`cue-before^vv

◎名 `cue-before@p, `cue-after@p
◎値
`uri$vt
`decibel$vt?
| `none$v
◎初 `none$v
◎適 すべての要素
◎継 されない
◎百 受容しない
◎算 指定されたとおり
◎順 文法に従う
◎表終

<p>
［
`cue-before$p ／ `cue-after$p
］~propは、
`聴覚-~box~model$の中で，要素の［
前／後
］に再生されることになる，聴感~icon
（すなわち，録音-済みな, あるいは生成-済みな音~clip）
を指定する。
◎
The cue-before and cue-after properties specify auditory icons (i.e. pre-recorded / pre-generated sound clips) to be played before (or after) the element within the aural box model.
</p>

<p class="note">注記：
この~propが供する機能性は，
`~SSML$の `audio$sS 要素に関係して現れ得るが、
事実，大きな不一致がある。
例えば、
`聴覚-~box~model$により，音声~cueは要素の出力音量~levelに結付けられることになるので、
この仕様の聴感~iconが供する機能性は，~SSMLの `audio^e 要素に比べ制限される。
◎
Note: Although the functionality provided by this property may appear related to the audio element from the SSML markup language [SSML], there are in fact major discrepancies. For example, the aural box model means that audio cues are associated to the  element’s volume level, and CSS Speech’s auditory icons provide limited functionality compared to SSML’s audio element.
</p>

<dl class="valdef">
	<dt id="valdef-cue-before-uri">`url$t</dt>
	<dd>
聴感~icon資源として指名される~URL。
~UAが指定された聴感~iconを音声化し得ない場合
（例：~file資源が見つからない, 未~supportな音声~codecなど）、
~bell音などの代替な~cueを生産することが推奨される。
◎
The URI designates an auditory icon resource. When a user agent is not able to render the specified auditory icon (e.g. missing file resource, or unsupported audio codec), it is recommended to produce an alternative cue, such as a bell sound.
</dd>

	<dt>`none@v</dt>
	<dd>
利用される聴感~iconは無いことを指定する。
◎
Specifies that no auditory icon is used.
</dd>

	<dt id="valdef-cue-before-decibel">`decibel$t</dt>
	<dd>
これは、
`聴覚-~box~model$の中での，要素の `voice-volume$p ~propの算出d値に相対的な（正なまたは負な）変化を表現する
（その結果、
音声~cueの出力音量~levelは `voice-volume$p ~propの変化に伴って変化する）。
省略された場合の暗黙の値は `0dB^v に算出される。
◎
Represents a change (positive or negative) relative to the computed value of the voice-volume property within the aural box model of the selected element. (As a result, the volume level of an audio cue changes when the voice-volume property changes). When omitted, the implied value computes to 0dB.
</dd>
	<dd>
`voice-volume$p ~propの算出d値が `silent^v （無音）の場合、
音声~cueも無音にされる
（この指定 `decibel$t 値に関わらず）。
他の場合の `voice-volume$p 値は、
常に，出力音量~level~keyword
（ `voice-volume$p の定義を見よ）
に相対的に指定される。
この出力音量~levelは、
利用者により “選好される” 聴感音量~設定群に較正された尺度に対応付けられる。
`voice-volume$p の継承d値に~decibel~offsetがすでに含まれている場合、
音声~cueに特有な dB ~offsetが加法的に組合される。
◎
When the computed value of the voice-volume property is silent, the audio cue is also set to silent (regardless of this specified &lt;decibel&gt; value). Otherwise (when not silent), voice-volume values are always specified relatively to the volume level keywords (see the definition of voice-volume), which map to a user-calibrated scale of "preferred" loudness settings. If the inherited voice-volume value already contains a decibel offset, the dB offset specific to the audio cue is combined additively.
</dd>
	<dd class="note">注記：
`voice-volume$p を通して出力音量が無音にされた音声~cueは，
再生されつつ（大きさのある）音は生成されなかったかのように同じ時間を占める一方で、
値 `none$v にされた音声~cueは，それ自体が無かったことにされる
（すなわち，`聴覚-次元$の中で~cueに割振られる時間は無い）。
◎
Note: There is a difference between an audio cue whose volume is set to silent and one whose value is none. In the former case, the audio cue takes up the same time as if it had been played, but no sound is generated. In the latter case, the there is no manifestation of the audio cue at all (i.e. no time is allocated for the cue in the aural dimension).
</dd>
</dl>

<div class="example">
<p>
~prop値の例：
◎
Examples of property values:
</p>

<pre class="lang-css">
a {
  cue-before: url(/audio/bell.aiff) -3dB;
  cue-after: url(dong.wav);
}

h1 {
  cue-before: url(../clips-1/pop.au) +6dB;
  cue-after: url(../clips-2/pop.au) 6dB;
}

div.caution { cue-before: url(./audio/caution.wav) +8dB; }
</pre>

</div>

		</section>
		<section id="cue-props-volume">
<h3 title="Relation between audio cues and speech synthesis volume levels">10.2. 音声~cueと発話~合成における出力音量~levelの相関</h3>

◎非規範的

<p>
`聴覚-~box~model$の中では、
要素の［
音声~cueと発話~合成
］の出力音量~levelは，相関する。
例えば，［
出力音量~levelが（ `decibel$t 値の指定により） `+0dB^v に設定されたとき
］の音声~cueの効果は、［
その再生~時に知覚される聴感音量
］が `voice-volume$p ~propの算出d値から決まる［
要素の発話~合成による音声化nの聴感音量
］に近くなることが欲される。
`voice-volume$p ~propに対する算出d値 `silent^v による結果は、
“強制的” に
（すなわち，音声~cueに指定された `decibel$t 値に関わらず）
無音にされた音声~cueになることに注意。
◎
The volume levels of audio cues and of speech synthesis within the aural box model of a selected element are related. For example, the desired effect of an audio cue whose volume level is set at +0dB (as specified by the &lt;decibel&gt; value) is that its perceived loudness during playback is close to that of the speech synthesis rendition of the selected element, as dictated by the computed value of the voice-volume property. Note that a silent computed value for the voice-volume property results in audio cues being "forcefully" silenced as well (i.e. regardless of the specified audio cue &lt;decibel&gt; value)
</p>

<div class="p">
<p>
`voice-volume$p ~propに対する~keyword値による出力音量は、
著作-時には知り得ない利用者の要件（例：聴感~環境, 個人的な選好）に合致するよう，
利用者により較正される。
したがって，上述したように［
音声~cueと発話~合成
］の聴感音量を近似的に揃えるためには、
作者は，［
音声~cueの（`平均^em†1 ）出力音量~level
］が［
“代表的な” 聴取~条件†2 の下での利用が意図された `voice-family$p
］に基づく［
発話~合成による音声化nの出力
］に合致することを確保するべきである。
発話~処理器には，［
生成される~TTS音声の波形~振幅を直に制御する能力
］があり、
~UAは，［
音声~cueの出力音量を調整できる†3
］ので、
これは，［
利用者により較正された出力音量~level†4
］に相対的な［
聴覚-~box~model内の［
~TTSと~cue
］音声~stream両者の聴感音量
］を実装が管理することを可能化するための基底線を設定する。
</p>

<ul>
	<li>†1
— 音声~streamにおける，抑揚, 瞬時の~~強勢（ `stress^en ）, 等々の変化により、
知覚される聴感音量には，ばらつきが生じ得るので。
</li>
	<li>†2
— すなわち、
周波数~spectrumに渡り 中央に均等化された（ `equalization^en ），
既定の~system出力音量~level。
</li>
	<li>†3
— すなわち、
~digital化された音~clipの内在的な波形~振幅に基づいて音声~信号を増幅させたり減衰させる。
</li>
	<li>†4
— `voice-volume$p ~propにて定義される~keywordを見よ。
</li>
</ul>

◎
The volume keywords of the voice-volume property are user-calibrated to match requirements not known at authoring time (e.g. auditory environment, personal preferences). Therefore, in order to achieve this approximate loudness alignment of audio cues and speech synthesis, authors should ensure that the volume level of audio cues (on average, as there may be discrete variations of perceived loudness due to changes in the audio stream, such as intonation, stress, etc.) matches the output of a speech synthesis rendition based on the voice-family intended for use, given "typical" listening conditions (i.e. default system volume levels, centered equalization across the frequency spectrum). As speech processors are capable of directly controlling the waveform amplitude of generated text-to-speech audio, and because user agents are able to adjust the volume output of audio cues (i.e. amplify or attenuate audio signals based on the intrinsic waveform amplitude of digitized sound clips), this sets a baseline that enables implementations to manage the loudness of both TTS and cue audio streams within the aural box model, relative to user-calibrated volume levels (see the keywords defined in the voice-volume property).
</div>

<p>
［
知覚される音声~特性（例：聴感音量）
］と［
~digital化された音声~信号に適用される処理（例：信号~圧縮）
］の間には複階的な関係性があるので、
ここでは，減衰が概して［
0dB（~clipping-thresholdに近い最大な音声~入力）以上 −60dB （全くの無音）以下
］の範囲に入る~decibel単位で指示されるような，単純な局面を想定する。
この文脈の下では、
“標準的な” 音声~clipは，これらの値の間を変動することになる。
聴感音量が最大になる~peak~levelは，（歪みを避けるために） −3dB 近くにされ、
それに伴う音声節の平均（ RMS ）出力音量~levelは，アリな限り高くされる
（すなわち，増幅の際の背景雑音を避けるよう静か過ぎない程度にされる）。
これにより，聴取者が体験する音声は、
概ね，~TTS出力と継ぎ目無く結合できるようになろう
（すなわち、
録音-済みな音声と発話~合成とを切り替える際の出力音量~levelの相違が，
聴き分けられなくなる程になろう）。
そのような慣行を~supportする業界~標準は存在しないが、
各種~TTS~engineは，［
利得や減衰が指定されていないときは，比較的~強めの音声~信号を生成する
］傾向にある。
~voiceや穏やかな音楽~用には、
−15dB RMS が~~相応な標準になると見受けられる。
◎
Due to the complex relationship between perceived audio characteristics (e.g. loudness) and the processing applied to the digitized audio signal (e.g. signal compression), we refer to a simple scenario whereby the attenuation is indicated in decibels, typically ranging from 0dB (i.e. maximum audio input, near clipping threshold) to -60dB (i.e. total silence). Given this context, a "standard" audio clip would oscillate between these values, the loudest peak levels would be close to -3dB (to avoid distortion), and the relevant audible passages would have average (RMS) volume levels as high as possible (i.e. not too quiet, to avoid background noise during amplification). This would roughly provide an audio experience that could be seamlessly combined with text-to-speech output (i.e. there would be no discernible difference in volume levels when switching from pre-recorded audio to speech synthesis). Although there exists no industry-wide standard to support such convention, different TTS engines tend to generate comparably-loud audio signals when no gain or attenuation is specified. For voice and soft music, -15dB RMS seems to be pretty standard.
</p>

		</section>
		<section id="cue-props-cue">
<h3 title="The cue shorthand property">10.3. `cue^p 略式~prop</h3>

◎名 `cue@p
◎値
`cue-before$tp `cue-after$tp?
◎初 個々の~propを見よ
◎適 すべての要素
◎継 されない
◎百 受容しない
◎算 個々の~propを見よ
◎順 文法に従う
◎表終

<p>
`cue$p ~propは、［
`cue-before$p, `cue-after$p
］用の略式~propである。
2 個の値は、
順に，これらの~propの値を与える。
値が 1 個だけ与えられた場合、
それが 2 個目の値も与える。
◎
The cue property is a shorthand for cue-before and cue-after. If two values are given the first value is cue-before and the second is cue-after. If only one value is given, it applies to both properties.
</p>

<div class="example">
<p>
略式~記法の例：
◎
Example of shorthand notation:
</p>

<pre class="lang-css">
h1 {
  cue-before: url(pop.au);
  cue-after: url(pop.au);
}
/* <span class="comment">
は、
次と等価になる：
◎
...is equivalent to:
</span> */
h1 {
  cue: url(pop.au);
}
</pre>
</div>

		</section>
	</section>
	<section id="voice-char-props">
<h2 title="Voice characteristic properties">11. ~voice特性~prop</h2>

		<section id="voice-props-voice-family">
<h3 title="The voice-family property">11.1. `voice-family^p ~prop</h3>

`voice-family^vv

◎名 `voice-family@p
◎値
[[`family-name$t | `generic-voice$t],]* [`family-name$t | `generic-voice$t] | `preserve$v
◎初
実装に依存する
◎
implementation-dependent
◎適 すべての要素
◎継 される
◎百 受容しない
◎算 指定されたとおり
◎順 文法に従う
◎表終

<p>
`voice-family$p ~propは、
~commaで分離された［
ある代替を指示する成分~値
］たちが成す~listを優先度~順に指定する
（これは、
視覚-~stylesheetにおける `font-family$p に相似的である）。
各 成分~値は、
合致するための判定基準を指定することにより，
発話~合成~用の~voice~instanceになり得るものと指名される。
この論題については、
`§ ~voice選定＠#voice-selection$を見よ。
◎
The voice-family property specifies a prioritized list of component values that are separated by commas to indicate that they are alternatives. (This is analogous to font-family in visual style sheets.) Each component value potentially designates a speech synthesis voice instance, by specifying match criteria. See the voice selection section on this topic.
</p>

<pre class="prod">
`generic-voice@t
	= `age$t? `gender$t `integer$vt?
</pre>

<p class="note">注記：
この~propが供する機能性は，
`~SSML$の `voice$sS 要素に類似するが、
この仕様は，~SSMLによる精巧な~voice自然言語~選択に等価なものは供さない。
この技術的な制限は，この~moduleの将来の改訂においては緩和され得る。
◎
Note: Although the functionality provided by this property is similar to the voice element from the SSML markup language [SSML], CSS Speech does not provide an equivalent to SSML’s sophisticated voice language selection. This technical limitation may be alleviated in a future revision of the Speech module.
</p>

<dl class="valdef">
	<dt id="valdef-voice-family-family-name">`family-name$t</dt>
	<dd>
値は，特定の~voice~instance
（例：
`Mike^v, `comedian^v, `mary^v, `carlos2^v, `"valley girl"^v, 等々）。
`font-family$p 名と同様に，各~voice名は、
次のいずれかとして，与えなければナラナイ
⇒＃
文字列（引用符付き，【 `string$t 】【！~INFRA#string】）／
`~CSS識別子$（引用符無し，【 `custom-ident$t 】）たちが成す【空白で分離された】連列
◎
Values are specific voice instances (e.g., Mike, comedian, mary, carlos2, "valley girl"). Like font-family names, voice names must either be given quoted as strings, or unquoted as a sequence of one or more CSS identifiers.
</dd>
	<dd class="note">注記：
その結果，引用符無し~voice名においては、
各~tokenの先頭にある ほとんどの約物や数字は，~escapeしなければならないことになる。
◎
Note: As a result, most punctuation characters, or digits at the start of each token, must be escaped in unquoted voice names.
</dd>
	<dd>
1 個の~voice名として，識別子の連列が与えられた場合、
その算出d値は，連列の中の各~識別子を［
1 個の~spaceを挟んで連結した文字列
］に変換して得られる名前になる。
◎
If a sequence of identifiers is given as a voice name, the computed value is the name converted to a string by joining all the identifiers in the sequence by single spaces.
</dd>
	<dd>
~voice名のうち，［
性別（ `gender$t ）~keyword（ `male^v, `female^v, `neutral^v ）と同じになるもの／
`~CSS全域~keyword$とたまたま合致するもの／
将来~利用に予約-済みな~keyword `default^v
］は、
これらの~keywordと区別できるよう引用符付きでなければナラナイ。
◎
Voice names that happen to be the same as the gender keywords (male, female and neutral) or that happen to match the CSS-wide keywords or preserve must be quoted to disambiguate with these keywords. The keyword default is reserved for future use and must also be quoted when used as voice names.
</dd>
	<dd class="note">注記：
`~SSML$においては、
~voice名は，~space等で互いに分離される
— したがって，空白~文字を包含し得ない。
◎
Note: In [SSML], voice names are space-separated and cannot contain whitespace characters.
</dd>
	<dd>［
空白, 数字, ［
~hyphen以外の約物
］］のいずれかを含んでいる~voice名は、
~codeの明確さを改善するため，引用符無しな形が妥当な場合でも, 引用符付きにすることが推奨される。
例えば：
<samp class="css">`voice-family^p: `"john doe", "Henry the-8th"^v;</samp>
◎
It is recommended to quote voice names that contain white space, digits, or punctuation characters other than hyphens—even if these voice names are valid in unquoted form—in order to improve code clarity. For example: voice-family: "john doe", "Henry the-8th";
</dd>

	<dt>`age@t</dt>
	<dd>
~keyword［
`child^v,
`young^v,
`old^v
］の いずれかを値にとり，~voice選定において合致が選好される年齢層を指示する。
◎
Possible values are child, young and old, indicating the preferred age category to match during voice selection.
</dd>
	<dd class="note">注記：
`~SSML$における年齢との対応付けには［
`child^v = 6 歳,
`young^v = 24 歳, 
`old^v = 75 歳【！y/o = years old】
］が推奨される。
処理器に依存する~voice照合~algoは、
より柔軟な年齢~範囲を利用してもヨイ。
◎
Note: A recommended mapping with [SSML] ages is: child = 6 y/o, young = 24 y/o, old = 75 y/o. More flexible age ranges may be used by the processor-dependent voice-matching algorithm.
</dd>

	<dt>`gender@t</dt>
	<dd>
~keyword［
`male^v, `female^v, `neutral^v
］のうちいずれか。
順に，［
男性, 女性, 中性
］の~voiceを指定する。
◎
One of the keywords male, female, or neutral, specifying a male, female, or neutral voice, respectively.
</dd>
	<dd class="note">注記：
個人の年齢や性別と, 認識-可能な~voiceの種別との関係度の解釈は、
実質的にいくつもの判定基準（文化的, 言語的, 生物学的, 等々）に依存するので、
現実的には普遍的な方式で定義し得ない。
したがって，この仕様が供する機能性は、
ある程度の誤差と引き換えに，幅広い発話~文脈にほどよく適用し得るような 単純~化された~modelを表現する。
この仕様の将来~versionでは、
発話~処理器~実装がもっと標準~化されるに伴い，~voice照合~algoはより精緻化され得る。
◎
Note: The interpretation of the relationship between a person’s age or gender, and a recognizable type of voice, cannot realistically be defined in a universal manner as it effectively depends on numerous criteria (cultural, linguistic, biological, etc.). The functionality provided by this specification therefore represent a simplified model that can be reasonably applied to a broad variety of speech contexts, albeit at the cost of a certain degree of approximation. Future versions of this specification may refine the level of precision of the voice-matching algorithm, as speech processor implementations become more standardized.
</dd>

	<dt id="valdef-voice-family-integer">`integer$t</dt>
	<dd>
選好される変種を指示する整数
（例： “男性かつ子供の~voiceのうち `2 個目^em のもの” ）。
正な整数のみが許容される。
値 `1^v は、
合致するすべての~voiceの中で 1 個目のものを指す。
◎
An integer indicating the preferred variant (e.g. "the second male child voice"). Only positive integers (i.e. excluding zero) are allowed. The value 1 refers to the first of all matching voices.
</dd>

	<dt>`preserve@v</dt>
	<dd>
内容~markupにより生じ得る自然言語の変化に関わらず，継承による
`voice-family$p 値の利用を指示する
（~voice選定と自然言語の取扱いについては、
下の節を見よ）。
この値が`根~要素$に適用されるときは、
`inherit$v として挙動する。
◎
Indicates that the voice-family value gets inherited and used regardless of any potential language change within the content markup (see the section below about voice selection and language handling). This value behaves as inherit when applied to the root element.
</dd>
	<dd class="note">注記：
要素の子孫は、
他の 
`voice-family$p
値
（例：
名前（ `family-name$t ）, 性別（ `gender$t ）, 年齢（ `age$t ））
により明示的に上書きされない限り，
`preserve$v 値を自動的に継承する。
◎
Note: Descendants of the element automatically inherit the preserve value, unless it is explicitly overridden by other voice-family values (e.g. name, gender, age).
</dd>
</dl>

<div class="example">
<p>
無効な宣言の例：
◎
Examples of invalid declarations:
</p>

<pre class="lang-css">
voice-family: john/doe;
    /* <span class="comment">
~forward-slashは~escapeされるべきである
◎
forward slash character should be escaped
</span> */
voice-family: john "doe";
    /* <span class="comment">
一連の識別子は文字列を含み得ない
◎
identifier sequence cannot contain strings
</span> */
voice-family: john!;
    /* <span class="comment">
感嘆符は~escapeされるべきである
◎
exclamation mark should be escaped
</span> */
voice-family: john@doe;
    /* <span class="comment">
“at” 文字（ `@^c ）は~escapeされるべきである
◎
"at" character should be escaped
</span> */
voice-family: #john;
    /* <span class="comment">
識別子は~hash文字から開始できない
◎
identifier cannot start with hash character
</span> */
voice-family: john 1st;
    /* <span class="comment">
識別子は数字から開始できない
◎
identifier cannot start with digit
</span> */
</pre>
</div>

			<section id="voice-selection">
<h4 title="Voice selection, content language">11.1.1. ~voice選定, 内容の自然言語</h4>

<p>
`voice-family$p ~propは、
発話~合成~用の~voice~instanceの選択を手引きするために利用される。
発話~能力がある~UAは、
この選択~処理の一部に，~markup内容における要素の自然言語【`内容~言語$】も織り込むモノトスル。
`voice-family$p ~prop値が子孫~要素に継承される際に，内容の深層へ伝わる［
名前（ `family-name$t ）,
性別（ `gender$t ）,
年齢（ `age$t ）,
および選好される “変種” （ `integer$t ~index）
］は、
~voice選定~用の~hintである。
内容~構造~内のどこであれ、
自然言語は，指定された~CSS~voice特性より優先される
（すなわち，より優先度が高い）。
◎
The voice-family property is used to guide the selection of the speech synthesis voice instance. As part of this selection process, speech-capable user agents must also take into account the language of the selected element within the markup content. The "name", "gender", "age", and preferred "variant" (index) are voice selection hints that get carried down the content hierarchy as the voice-family property value gets inherited by descendant elements. At any point within the content structure, the language takes precedence (i.e. has a higher priority) over the specified CSS voice characteristics.
</p>

<p>
以下の~listに~voice選定~algoを要旨する
（方言による差異を吸収するため、
ここでの
“内容~言語” 【当の要素の内容の自然言語（`内容~言語$）】の定義は緩められていることに注意）：
◎
The following list outlines the voice selection algorithm (note that the definition of "language" is loose here, in order to cater for dialectic variations):
</p>

<ol>
	<li>
内容~言語に対し，可用な~voice~instanceが 1 個しかない場合、
指定された~CSS~voice特性に関わらず，この~voiceを利用するモノトスル。
◎
If only a single voice instance is available for the language of the selected content, then this voice must be used, regardless of the specified CSS voice characteristics.
</li>
	<li>
内容~言語に対し，複数の~voice~instanceが可用な場合、
選ばれる~voiceは，
指定された名前, または［
性別, 年齢, ~voiceに選好される変種
］に最も近く合致するものになる。
“最良~合致” の実際の定義は，処理器に依存する。
例えば，大人の男性／女性の~voiceのみが可用な~systemにおいては、
"<samp class="css">`voice-family$p: `young male^v</samp>"
に対しては、
音高が高い女性~voiceの方が ほどよく合致するであろう
— その方が調子は少年に近くなろうから。
供された どの `voice-family$p 成分~値にも特性が合致する~voice~instanceが無い場合、
（内容~言語に適するものの中で）最初に【どのような順序か不明】可用な~voice~instanceを利用するモノトスル。
◎
If several voice instances are available for the language of the selected content, then the chosen voice is the one that most closely matches the specified name, or gender, age, and preferred voice variant. The actual definition of "best match" is processor-dependent. For example, in a system that only has male and female adult voices available, a reasonable match for "voice-family: young male" may well be a higher-pitched female voice, as this tone of voice would be closer to that of a young boy. If no voice instance matches the characteristics provided by any of the voice-family component values, the first available voice instance (amongst those suitable for the language of the selected content) must be used.
</li>
	<li>
内容~言語に利用し得る~voiceが無い場合、
~UAには，適切な~TTS~voiceが無いことを利用者に知らせることが推奨される。
◎
If no voice is available for the language of the selected content, it is recommended that user agents let the user know about the lack of appropriate TTS voice.
</li>
</ol>

<p>
内容~flowの中で~CSS~voice特性が変化するようなどの場所でも、
発話~合成器~voiceを評価し直すモノトスル
（すなわち，選択~処理を再び行うモノトスル）。
`preserve$v ~keywordが利用されていない限り、
内容~言語が変化する度に，~voiceも計算し直すモノトスル
（ `preserve$v は、
下にデモられるとおり，［
内容~言語~用に設計されたものでない~voice
］を利用して［
埋込まれた外国語~text
］を発話させたいときに有用になり得る）。
◎
The speech synthesizer voice must be re-evaluated (i.e. the selection process must take place once again) whenever any of the CSS voice characteristics change within the content flow. The voice must also be re-calculated whenever the content language changes, unless the preserve keyword is used (this may be useful in cases where embedded foreign language text can be spoken using a voice not designed for this language, as demonstrated by the example below).
</p>

<p class="note">注記：
~voiceの動的な算出は，不慮の~lagをもたらし得るので、
~UAは再生を開始する前に，文書~treeの中の具象的な~voice~instanceの解決を試みるべきである。
◎
Note: Dynamically computing a voice may lead to unexpected lag, so user agents should try to resolve concrete voice instances in the document tree before the playback starts.
</p>

<div class="example">
<p>
~prop値の例：
◎
Examples of property values:
</p>

<pre class="lang-css">
h1 { voice-family: announcer, old male; }
p.romeo  { voice-family: romeo, young male; }
p.juliet { voice-family: juliet, young female; }
p.mercutio { voice-family: young male; }
p.tybalt { voice-family: young male; }
p.nurse { voice-family: amelie; }
</pre>
…
<pre class="lang-html">
&lt;p class="romeo" xml:lang="en-US"&gt;

<span class="comment">
次の~French~textは~Englishの~voiceで発話されることになる：
◎
The French text below will be spoken with an English voice:
</span>
  &lt;span style="voice-family: preserve;" xml:lang="fr-FR"&gt;Bonjour monsieur !&lt;/span&gt;

<span class="comment">
次の英文~textは，（親の `p^e 要素から継承されるもの）~class "`romeo^v" に対応する~voiceとは異なる~voiceで発話される：
◎
The English text below will be spoken with a voice different than that corresponding to the class "romeo" (which is inherited from the "p" parent element):
</span>
  &lt;span style="voice-family: female;"&gt;Hello sir!&lt;/span&gt;
&lt;/p&gt;
</pre>
</div>

			</section>
		</section>
		<section id="voice-props-voice-rate">
<h3 title="The voice-rate property">11.2. `voice-rate^p ~prop</h3>

`voice-rate^vv

◎名 `voice-rate@p
◎値
[`normal$v
| `x-slow$v
| `slow$v
| `medium$v
| `fast$v
| `x-fast$v]
|| `percentage [0,∞]$vt
◎初 `normal$v
◎適 すべての要素
◎継 される
◎百
既定~値に相対的
◎
refer to default value
◎算
［
~keyword値と, ~keywordに相対的な 100% でない百分率（~option）
］の組
◎
a keyword value, and optionally also a percentage relative to the keyword (if not 100%)
◎順 文法に従う
◎表終

<p>
`voice-rate$p ~propは、
毎分あたりの語数により，生成される合成な発話の速度を操作する。
◎
The voice-rate property manipulates the rate of generated synthetic speech in terms of words per minute.
</p>

<p class="note">注記：
この~propが供する機能性は、
`~SSML$の `prosody$sS 要素の `rate^a 属性に類似するが，留意すべき不一致がある。
例えば，この~propに対する［
速度~keyword値,
百分率~値による修飾
］は、
要素に継承された値と組合せる方法があることに因り，排他的でない。
◎
Note: Although the functionality provided by this property is similar to the rate attribute of the prosody element from the SSML markup language [SSML], there are notable discrepancies. For example, CSS Speech rate keywords and percentage modifiers are not mutually-exclusive, due to how values are inherited and combined for selected elements.
</p>

<dl class="valdef">
	<dt>`normal@v</dt>
	<dd>
発話~合成器が現在~作動中の~voiceを生産するときの，既定の速度を表現する。
これは、
処理器に特有であり，［
自然言語と方言, ~voiceの “個性”
］に依存する。
◎
Represents the default rate produced by the speech synthesizer for the currently active voice. This is processor-specific and depends on the language and dialect, and on the "personality" of the voice.
</dd>

	<dt>`x-slow@v</dt>
	<dt>`slow@v</dt>
	<dt>`medium@v</dt>
	<dt>`fast@v</dt>
	<dt>`x-fast@v</dt>
	<dd>
これらは 実装／~voiceに特有な発話速度であり、
後に挙げたものほど，速くなる（厳密には，より遅くない）【！non-decreasing】。
例えば，~English用の代表的な値は（毎分あたりの語数で）［
`x-slow^v = 80 語,
`slow^v = 120 語,
`medium^v = 180 語 〜 200 語,
`fast^v = 500 語
］になる。
◎
A sequence of monotonically non-decreasing speaking rates that are implementation- and voice-specific. For example, typical values for the English language are (in words per minute) x-slow = 80, slow = 120, medium = between 180 and 200, fast = 500.
</dd>

	<dt id="valdef-voice-rate-percentage">`percentage [0,∞]$t</dt>
	<dd>
<p>
負でない百分率~値のみ許容され、
次の値に相対的な変化を表現する：
</p>

		<ol>
			<li>
（上に列挙された）~keyword値も与えられていれば それ。
</li>
			<li>
他の場合，`根~要素$に対しては既定~値
【すなわち，初期~値】。
</li>
			<li>
他の場合，継承されている発話速度
（それ自身も~keyword値と百分率の組合nになり得る
— この場合の率【！百分率】は，累積される）。
</li>
		</ol>

<p>
例えば 50% は、
発話速度が 0.5 倍されることを意味する。
100% を上回る百分率においては，発話速度が（基底の~keywordに比して）より速くなり、
100% を下回る百分率においては，よりゆっくりになる。
</p>

◎
Only non-negative percentage values are allowed. This represents a change relative to the given keyword value (see enumeration above), or to the default value for the root element, or otherwise to the inherited speaking rate (which may itself be a combination of a keyword value and of a percentage, in which case percentages are combined multiplicatively). For example, 50% means that the speaking rate gets multiplied by 0.5 (half the value). Percentages above 100% result in faster speaking rates (relative to the base keyword), whereas percentages below 100% result in slower speaking rates.
</dd>
</dl>

<div class="example">
<p>
継承d値の例：
◎
Examples of inherited values:
</p>

<pre class="lang-xml">
&lt;body&gt;
  &lt;e1&gt;
    &lt;e2&gt;
      &lt;e3&gt;
        …
      &lt;/e3&gt;
    &lt;/e2&gt;
  &lt;/e1&gt;
&lt;/body&gt;
</pre>
…
<pre class="lang-css">
body { voice-rate: inherit; }
    /* <span class="comment">
初期~値は `normal$v
（実際の発話速度の値は作動中の~voiceに依存する）
◎
the initial value is 'normal' (the actual speaking rate value depends on the active voice)
</span> */

e1 { voice-rate: +50%; }
    /* <span class="comment">
算出d値は［
`normal$v と `50%^v
］の組。
これは， `normal$v の 0.5 倍（発話速度の半分）に対応する速度に解決されることになる。
◎
the computed value is ['normal' and 50%], which will resolve to the rate corresponding to 'normal' multiplied by 0.5 (half the speaking rate)
</span> */

e2 { voice-rate: fast 120%; }
    /* <span class="comment">
算出d値は［
`fast$v と `120%^v
］の組。
これは， `fast$v の 1.2 倍に対応する速度に解決されることになる。
◎
the computed value is ['fast' and 120%], which will resolve to the rate corresponding to 'fast' multiplied by 1.2
</span> */

e3 {
    voice-rate: normal;
    /* <span class="comment">
発話速度を内在的な~voice値に “再設定-” する。
算出d値は `normal$v になる
（実際の値については，次の~commentを見よ）。
◎
"resets" the speaking rate to the intrinsic voice value, the computed value is 'normal' (see comment below for actual value)
</span> */

    voice-family: "another-voice";
    /* <span class="comment">
~voiceが別のものにされているので、
計算される発話速度は， `body^e のそれに比して
（ `voice-rate$p の算出d値が同じであっても）
変わり得る。
◎
because the voice is different, the calculated speaking rate may vary compared to "body" (even though the computed 'voice-rate' value is the same)
</span> */
}
</pre>
</div>

		</section>
		<section id="voice-props-voice-pitch">
<h3 title="The voice-pitch property">11.3. `voice-pitch^p ~prop</h3>

`voice-pitch^vv

◎名 `voice-pitch@p
◎値
`frequency$vt &amp;&amp; `absolute$v
| [[`x-low$v
| `low$v
| `medium$v
| `high$v
| `x-high$v]
|| [`frequency$vt | `semitones$vt | `percentage$vt]]
◎初 `medium$v
◎適 すべての要素
◎継 される
◎百
継承d値に相対的
◎
refer to inherited value
◎算
定義済みな音高~keywordのみが指定された場合、
その~keyword。
他の場合、
~keyword値（もしあれば）を［
現在の `voice-family$p に基づいて
— 相対~offsetが指定されていれば それも適用した上で —
固定的な周波数に変換する
］ことにより計算される，絶対的な周波数。
◎
one of the predefined pitch keywords if only the keyword is specified by itself, otherwise an absolute frequency calculated by converting the keyword value (if any) to a fixed frequency based on the current voice-family and by applying the specified relative offset (if any)
◎順 文法に従う
◎表終

<p>
`voice-pitch$p ~propは、
生成される発話~出力の “基底線” とされる音高を指定する。
これは、
使用 `voice-family$p ~instanceに依存し，発話~合成~処理器ごとに変わり得る
（およそ，出力の平均~音高に対応する）。
例えば、
男性~voiceに共通的な音高は 120Hz 周辺になる一方，女性~voiceでは 210Hz 周辺になる。
◎
The voice-pitch property specifies the "baseline" pitch of the generated speech output, which depends on the used voice-family instance, and varies across speech synthesis processors (it approximately corresponds to the average pitch of the output). For example, the common pitch for a male voice is around 120Hz, whereas it is around 210Hz for a female voice.
</p>

<p class="note">注記：
この~propが供する機能性は，
`~SSML$の `prosody$sS 要素の `pitch^a 属性に類似するが、
留意すべき不一致がある。
例えば，この~propに対する［
音高~keyword値,
相対~変化~値（周波数, 半音数, 百分率）
］は、
要素に継承された値と組合せる方法があることに因り，排他的でない。
◎
Note: Although the functionality provided by this property is similar to the pitch attribute of the prosody element from the SSML markup language [SSML], there are notable discrepancies. For example, CSS Speech pitch keywords and relative changes (frequency, semitone or percentage) are not mutually-exclusive, due to how values are inherited and combined for selected elements.
</p>

<dl class="valdef">
	<dt id="valdef-voice-pitch-frequency">`frequency$t</dt>
	<dd>
周波数~単位（~Hertzまたは~kiloHertz）による値
（例： `100Hz^v, `+2kHz^v ）。
ただし，
`absolute$v ~keywordが指定された下での値は、
正な数に制約される。
他の場合の［
負な／正な
］値は，継承d値に相対的な［
減分／増分
］を表現する。
例えば
`2kHz^v
は，正な~offset（ `+2kHz^v と厳密に等価）になり、
`+2kHz absolute^v は，絶対~周波数（ `2kHz absolute^v と厳密に等価）になる。
◎
A value in frequency units (Hertz or kiloHertz, e.g. 100Hz, +2kHz). Values are restricted to positive numbers when the absolute keyword is specified. Otherwise (when the absolute keyword is not specified), a negative value represents a decrement, and a positive value represents an increment, relative to the inherited value. For example, 2kHz is a positive offset (strictly equivalent to +2kHz), and +2kHz absolute is an absolute frequency (strictly equivalent to 2kHz absolute).
</dd>

	<dt>`absolute@v</dt>
	<dd>
指定された場合、
この~keywordは，指定された周波数が 絶対的な値を表現することを指示する。
負な周波数が指定された場合に算出される周波数は 0 になる。
◎
If specified, this keyword indicates that the specified frequency represents an absolute value. If a negative frequency is specified, the computed frequency will be zero.
</dd>

	<dt id="valdef-voice-pitch-semitones">`semitones$t</dt>
	<dd>
継承d値に相対的な，変化（減分または増分）を指定する。
`semitones@t
に許容される値の構文は、
単位~識別子 `st^u （半音数）を伴う`次元$である。
`半音@
区間は，~~半音による等分平均律の各 音階（音符と次の音符の間）に対応する。
したがって，`半音$は、
そのような音階の下で隣り合う 2 個の音高の周波数の相違として量られる。
1 個の半音で隔てられた 2 つの周波数の比率は，
2 の 12 乗根
（約 ( 11011 ~DIV 10393 ) ~EQ 1.0594631 ）
に（この桁数の精度で）等しい。
すなわち、
半音による~offsetに対応する~Hertz単位の値は，
その~offsetが適用される初期~周波数に相対的になる
（言い換えれば、
半音は，固定的な~Hertz単位の数値に対応しない）。
◎
Specifies a relative change (decrement or increment) to the inherited value. The syntax of &lt;semitones&gt; allowed values is a dimension with the unit identifier st (semitones). A semitone interval corresponds to the step between each note on an equal temperament chromatic scale. A semitone can therefore be quantified as the difference between two consecutive pitch frequencies on such scale. The ratio between two consecutive frequencies separated by exactly one semitone is the twelfth root of two (approximately 11011/10393, which equals exactly 1.0594631). As a result, the value in Hertz corresponding to a semitone offset is relative to the initial frequency the offset is applied to. (In other words, a semitone doesn’t correspond to a fixed numerical value in Hertz.)
</dd>

	<dt id="valdef-voice-pitch-percentage">`percentage$t</dt>
	<dd>
百分率~値には［
正な／負な
］値が許容され、
継承d値に相対的な［
増分／減分
］を表現する。
その算出d値は、
継承d値の指定された割合分を，継承d値に［
加算する／減算する
］ことにより計算される。
例えば，継承d値が `200Hz^v のとき、
`50%^v（ `+50%^v に等価）による結果は
200 + ( 200 × 0.5 ) = `300Hz^v
になり，
`-50%^v による結果は
200 − ( 200 × 0.5 ) = `100Hz^v
になる。
◎
Positive and negative percentage values are allowed, to represent an increment or decrement (respectively) relative to the inherited value. Computed values are calculated by adding (or subtracting) the specified fraction of the inherited value, to (from) the inherited value. For example, 50% (which is equivalent to +50%) with a inherited value of 200Hz results in 200 + (200*0.5) = 300Hz. Conversely, -50% results in 200-(200*0.5) = 100Hz.
</dd>

	<dt>`x-low@v</dt>
	<dt>`low@v</dt>
	<dt>`medium@v</dt>
	<dt>`high@v</dt>
	<dt>`x-high@v</dt>
	<dd>
これらは，［
実装, ~voice
］に特有な音高~levelを表す
— 後に挙げたものほど，高くなる（厳密には，より低くない）
【！non-decreasing】。
所与の要素に対する算出d値が~keywordのみの場合
（すなわち，相対~offsetは指定されていない）、
対応する絶対~周波数は，~voiceが変化する度に評価し直されることになる。
逆に、
相対~offsetの適用には，［
相対~offsetが指定された地点における現在の~voice
］に基づく周波数の計算が要求されるので、［
~styleの~cascadeにより ~voice変化が深層へ伝わる
］のに関わらず，絶対的に算出される周波数が継承されることになる。
したがって，作者は、
~voice変化に応じて［
~keywordから具象的な, ~voiceに依存する周波数への変換の再~評価
］を誘発させたいときは，~keyword値を利用するべきである。
◎
A sequence of monotonically non-decreasing pitch levels that are implementation and voice specific. When the computed value for a given element is only a keyword (i.e. no relative offset is specified), then the corresponding absolute frequency will be re-evaluated on a voice change. Conversely, the application of a relative offset requires the calculation of the resulting frequency based on the current voice at the point at which the relative offset is specified, so the computed frequency will inherit absolutely regardless of any voice change further down the style cascade. Authors should therefore only use keyword values in cases where they wish that voice changes trigger the re-evaluation of the conversion from a keyword to a concrete, voice-dependent frequency.
</dd>
</dl>

<p>
算出された絶対~周波数が負な場合、
0 ~Hertz以上に切上げられる。
発話~能力がある~UAは、
周波数の計算から得られ得る全~数値~範囲ではなく，
特定の範囲の値を~supportすることになると見込まれる。
したがって，~UAにおける実際の値は、
実装に依存する下限／上限に切詰められ得る。
例えば，周波数
`0Hz^v
は合法的に計算され得るが、
発話~合成器の文脈の下では，より有意義な値に切詰められるであろう。
◎
Computed absolute frequencies that are negative are clamped to zero Hertz. Speech-capable user agents are likely to support a specific range of values rather than the full range of possible calculated numerical values for frequencies. The actual values in user agents may therefore be clamped to implementation-dependent minimum and maximum boundaries. For example, although the 0Hz frequency can be legitimately calculated, it may be clamped to a more meaningful value in the context of the speech synthesizer.
</p>

<div class="example">
<p>
~prop値の例：
◎
Examples of property values:
</p>

<pre class="lang-css">
h1 { voice-pitch: 250Hz; }
    /* <span class="comment">
継承される絶対~周波数に相対的な正な~offset
◎
positive offset relative to the inherited absolute frequency
</span> */
h1 { voice-pitch: +250Hz; }
    /* <span class="comment">
前の行と同じになる
◎
identical to the line above
</span> */
h2 { voice-pitch: +30Hz absolute; }
    /* <span class="comment">
増分ではない
◎
not an increment
</span> */
h2 { voice-pitch: absolute 30Hz; }
    /* <span class="comment">
前の行と同じになる
◎
identical to the line above
</span> */
h3 { voice-pitch: -20Hz; }
    /* <span class="comment">
継承される絶対~周波数に相対的な負な~offset（減分）
◎
negative offset (decrement) relative to the inherited absolute frequency
</span> */
h4 { voice-pitch: -20Hz absolute; }
    /* <span class="comment">
違法な構文。
値は無視される
（ `absolute$v ~keywordは，負な周波数には許容されない）
◎
illegal syntax =&gt; value ignored ("absolute" keyword not allowed with negative frequency)
</span> */
h5 { voice-pitch: -3.5st; }
    /* <span class="comment">
半音数, 負な~offset
◎
semitones, negative offset
</span> */
h6 { voice-pitch: 25%; }
    /* <span class="comment">
これは “継承d値の 4 分の 1 を継承d値に加える” ことを意味する
◎
this means "add a quarter of the inherited value, to the inherited value"
</span> */
h6 { voice-pitch: +25%; }
    /* <span class="comment">
前の行と同じになる
◎
identical to the line above
</span> */
</pre>
</div>

		</section>
		<section id="voice-props-voice-range">
<h3 title="The voice-range property">11.4. `voice-range^p ~prop</h3>

`voice-range^vv
`voice-pitch^vv

◎名 `voice-range@p
◎値 `voice-pitch$tp
◎初 `medium$v
◎適 すべての要素
◎継 される
◎百 継承d値に相対的
◎
refer to inherited value
◎算 `voice-pitch$p の記述と同じ【なので省略する】
◎
one of the predefined pitch keywords if only the keyword is specified by itself, otherwise an absolute frequency calculated by converting the keyword value (if any) to a fixed frequency based on the current voice-family and by applying the specified relative offset (if any)
◎順 文法に従う
◎表終

<p>
`voice-range$p ~propは、
“基底線” 音高の可変度
— すなわち，基本周波数
【`fundamental frequency＠https://en.wikipedia.org/wiki/Fundamental_frequency$en】
が発話~出力の平均~音高からどの程度 外れられるか —
を指定する。
生成される発話の動的な音高~範囲は、
高度に活発な~voiceにおいては，一般に拡大する。
例えば，発話における意味や強勢を伝達するために，屈折
【`inflection＠https://en.wikipedia.org/wiki/Inflection$en, `~~屈折＠https://ja.wikipedia.org/wiki/%E8%AA%9E%E5%BD%A2%E5%A4%89%E5%8C%96$】
が利用されるときなど。
概して、
範囲が狭い場合は平坦で単調な~voiceを生産する一方，範囲が広い場合は活発な~voiceを生産する。
◎
The voice-range property specifies the variability in the "baseline" pitch, i.e. how much the fundamental frequency may deviate from the average pitch of the speech output. The dynamic pitch range of the generated speech generally increases for a highly animated voice, for example when variations in inflection are used to convey meaning and emphasis in speech. Typically, a low range produces a flat, monotonic voice, whereas a high range produces an animated voice.
</p>

<p class="note">注記：
この~propが供する機能性は，
`~SSML$の `prosody$sS 要素の `range^a 属性に類似するが、
留意すべき不一致がある。
例えば，この~propに対する［
音高~範囲~keyword値,
相対~変化~値（周波数, 半音数, 百分率など）
］は、
要素に継承された値と組合せる方法があることに因り，排他的でない。
◎
Note: Although the functionality provided by this property is similar to the range attribute of the prosody element from the SSML markup language [SSML], there are notable discrepancies. For example, CSS Speech pitch range keywords and relative changes (frequency, semitone or percentage) are not mutually-exclusive, due to how values are inherited and combined for selected elements.
</p>

<p>
【各種 成分~値の定義と付随する注釈は、 `voice-pitch$p のそれと一致するので，和訳は省略する。】
◎
&lt;frequency&gt;
• A value in frequency units (Hertz or kiloHertz, e.g. 100Hz, +2kHz). Values are restricted to positive numbers when the absolute keyword is specified. Otherwise (when the absolute keyword is not specified), a negative value represents a decrement, and a positive value represents an increment, relative to the inherited value. For example, 2kHz is a positive offset (strictly equivalent to +2kHz), and +2kHz absolute is an absolute frequency (strictly equivalent to 2kHz absolute).
◎
absolute
• If specified, this keyword indicates that the specified frequency represents an absolute value. If a negative frequency is specified, the computed frequency will be zero.
◎
&lt;semitones&gt;
• Specifies a relative change (decrement or increment) to the inherited value as a semitone.
◎
&lt;percentage&gt;
• Positive and negative percentage values represent an increment or decrement (respectively) relative to the inherited value. Computed values are calculated by adding (or subtracting) the specified fraction of the inherited value, to (from) the inherited value. For example, 50% (which is equivalent to +50%) with a inherited value of 200Hz results in 200 + (200*0.5) = 300Hz. Conversely, -50% results in 200-(200*0.5) = 100Hz.
◎
x-low, low, medium, high, x-high
• A sequence of monotonically non-decreasing pitch levels that are implementation and voice specific. When the computed value for a given element is only a keyword (i.e. no relative offset is specified), then the corresponding absolute frequency will be re-evaluated on a voice change. Conversely, the application of a relative offset requires the calculation of the resulting frequency based on the current voice at the point at which the relative offset is specified, so the computed frequency will inherit absolutely regardless of any voice change further down the style cascade. Authors should therefore only use keyword values in cases where they wish that voice changes trigger the re-evaluation of the conversion from a keyword to a concrete, voice-dependent frequency.
◎
Computed absolute frequencies that are negative are clamped to zero Hertz. Speech-capable user agents are likely to support a specific range of values rather than the full range of possible calculated numerical values for frequencies. The actual values in user agents may therefore be clamped to implementation-dependent minimum and maximum boundaries. For example: although the 0Hz frequency can be legitimately calculated, it may be clamped to a more meaningful value in the context of the speech synthesizer.
</p>

<div class="example">
<p>
継承d値の例：
◎
Examples of inherited values:
</p>

<pre class="lang-xml">
&lt;body&gt;
  &lt;e1&gt;
    &lt;e2&gt;
      &lt;e3&gt;
        &lt;e4&gt;
          &lt;e5&gt;
            &lt;e6&gt;
            …
            &lt;/e6&gt;
          &lt;/e5&gt;
        &lt;/e4&gt;
      &lt;/e3&gt;
    &lt;/e2&gt;
  &lt;/e1&gt;
&lt;/body&gt;
</pre>
…
<pre class="lang-css">
body { voice-range: inherit; }
    /* <span class="comment">
初期~値は `medium$v
（実際の周波数の値は現在の~voiceに依存する)
◎
the initial value is 'medium' (the actual frequency value depends on the current voice)
</span> */

e1 { voice-range: +25%; }
    /* <span class="comment">
算出d値は［
`medium$v + `25%^v
］であり，
( `medium$v に対応する周波数 ) + 0.25 × ( `medium$v に対応する周波数 )
に解決される
◎
the computed value is ['medium' + 25%] which resolves to the frequency corresponding to 'medium' plus 0.25 times the frequency corresponding to 'medium'
</span> */

e2 { voice-range: +10Hz; }
    /* <span class="comment">
算出d値は ( %FREQ + 10Hz ) 。
ここで %FREQ は，上の "`e1^css" 規則にて計算される絶対~周波数。
◎
the computed value is [FREQ + 10Hz] where "FREQ" is the absolute frequency calculated in the "e1" rule above.
</span> */

e3 {
    voice-range: inherit;
    /* <span class="comment">
これは省略し得るが，明確さのために明示的に指定されている
◎
this could be omitted, but we explicitly specify it for clarity purposes
</span> */

    voice-family: "another-voice";
    /* <span class="comment">
この~voice変化により， `body^e 要素から継承された初期~値の `medium$v ~keywordは評価し直される
（すなわち，~voiceに依存する~keyword値から具象的な絶対~周波数に変換される）
ことになるが、
相対~offsetは~styleの~cascadeにより深層へ伝わるので，
`voice-range$p の実際の継承d値は 上の "`e2^css" 規則にて計算される周波数になる。
◎
this voice change would have resulted in the re-evaluation of the initial 'medium' keyword inherited by the "body" element (i.e. conversion from a voice-dependent keyword value to a concrete, absolute frequency), but because relative offsets were applied down the style cascade, the inherited value is actually the frequency calculated at the "e2" rule above.
</span> */
}

e4 { voice-range: 200Hz absolute; }
    /* <span class="comment">
現在の~voiceに依存しない絶対~周波数で上書きする
◎
override with an absolute frequency which doesn’t depend on the current voice
</span> */

e5 { voice-range: 2st; }
    /* <span class="comment">
算出d値は、［
200Hz + 2 個の半音
］の計算から得られる，絶対~周波数
（実際の周波数は，それを適用する基底~値に依存する半音に対応することに留意）
◎
the computed value is an absolute frequency, which is the result of the calculation: 200Hz + two semitones (reminder: the actual frequency corresponding to a semitone depends on the base value to which it applies)
</span> */

e6 {
    voice-range: inherit;
    /* <span class="comment">
これは省略し得るが，明確さのために明示的に指定されている
◎
this could be omitted, but we explicitly specify it for clarity purposes
</span> */

    voice-family: "yet-another-voice";
    /* <span class="comment">
~voice変化にかかわらず，
`voice-range$p の算出d値は "`e5^css" に対するもの
（すなわち、
現在の~voiceから独立な，絶対~周波数による値）
と同じになる
◎
despite the voice change, the computed value is the same as for "e5" (i.e. an absolute frequency value, independent from the current voice)
</span> */
}
</pre>

</div>

		</section>
		<section id="voice-props-voice-stress">
<h3 title="The voice-stress property">11.5. `voice-stress^p ~prop</h3>

`voice-stress^vv

◎名 `voice-stress@p
◎値
`normal$v
| `strong$v
| `moderate$v
| `none$v
| `reduced$v
◎初 `normal$v
◎適 すべての要素
◎継 される
◎百 受容しない
◎算 指定されたとおり
◎順 文法に従う
◎表終

<p>
`voice-stress$p ~propは、［
音高の変化, ~timing変化, 聴感音量, 他の聴感上の相違
］の組合nを利用して，通常時に適用される強勢の強度を操作する。
したがって、
これらの値の精確な意味は，発話される自然言語に依存する。
◎
The voice-stress property manipulates the strength of emphasis, which is normally applied using a combination of pitch change, timing changes, loudness and other acoustic differences. The precise meaning of the values therefore depend on the language being spoken.
</p>

<p class="note">注記：
この~propが供する機能性は、
`~SSML$の`emphasis$sS 要素に類似する。
◎
Note: The functionality provided by this property is similar to the emphasis element from the SSML markup language [SSML].
</p>

<dl class="valdef">
	<dt>`normal@v</dt>
	<dd>
発話~合成器により生産される既定の強勢を表現する。
◎
Represents the default emphasis produced by the speech synthesizer.
</dd>

	<dt>`none@v</dt>
	<dd>
合成器が通常時には強勢する~textを強勢させなくする
◎
Prevents the synthesizer from emphasizing text it would normally emphasize.
</dd>

	<dt>`moderate@v</dt>
	<dt>`strong@v</dt>
	<dd>
これらの値は強度を表す。
後に挙げたものほど，強くなる
（厳密には，より弱くない）
【！non-decreasing】。
これらの適用の結果、
発話~合成器が通常時に生産する強勢
（すなわち， `normal$v に対応する値）は，より強められる。
◎
These values are monotonically non-decreasing in strength. Their application results in more emphasis than what the speech synthesizer would normally produce (i.e. more than the value corresponding to normal).
</dd>

	<dt>`reduced@v</dt>
	<dd>
【！Effectively】上とは逆に，単語の強勢を抑制する。
◎
Effectively the opposite of emphasizing a word.
</dd>
</dl>

  <div class="example">
<p>
~HTML見本を伴う~prop値の例：
◎
Examples of property values, with HTML sample:
</p>

<pre class="lang-css">
.default-emphasis { voice-stress: normal; }
.lowered-emphasis { voice-stress: reduced; }
.removed-emphasis { voice-stress: none; }
.normal-emphasis { voice-stress: moderate; }
.huge-emphasis { voice-stress: strong; }
</pre>
…
<pre class="lang-html">
&lt;p&gt;これは&lt;em&gt;ばかでかい&lt;/em&gt;車だ。&lt;/p&gt;
&lt;!-- <span class="comment">前の行†による発話~出力は，次の行と同じになる：

◎
The speech output from the line above is identical to the line below:
</span> --&gt;
&lt;p&gt;これは&lt;em class="default-emphasis"&gt;ばかでかい&lt;/em&gt;車だ。&lt;/p&gt;

&lt;p&gt;この車は&lt;em class="lowered-emphasis"&gt;ゴツい&lt;/em&gt;！&lt;/p&gt;
&lt;!-- <span class="comment">前の行の強勢は抑制されるのみである一方，次の "`em^e" は全く強勢されない：
◎
The "em" below is totally de-emphasized, whereas the emphasis in the line above is only reduced:
</span> --&gt;
&lt;p&gt;この車は&lt;em class="removed-emphasis"&gt;ゴツい&lt;/em&gt;！&lt;/p&gt;

&lt;!-- <span class="comment">下の 2 行は強勢~levelの増大をデモる：
◎
The lines below demonstrate increasing levels of emphasis:
</span> --&gt;
&lt;p&gt;これは&lt;em class="normal-emphasis"&gt;ばかでかい&lt;/em&gt;車だ！&lt;/p&gt;
&lt;p&gt;これは&lt;em class="huge-emphasis"&gt;ばかでかい&lt;/em&gt;車だ！！&lt;/p&gt;
</pre>

<p class="trans-note">【†
原文は `em^e による~markupが抜け落ちていると見受けられるので補完している。
】</p>

</div>
<!-- 
This is a big car.
This car is massive!
 -->

		</section>
	</section>
	<section id="duration-props">
<h2 title="Voice duration property">12. ~voice持続時間~prop</h2>

		<section id="mixing-props-voice-duration">
<h3 title="The voice-duration property">12.1. `voice-duration^p ~prop</h3>

`voice-duration^vv

◎名 `voice-duration@p
◎値 `auto$v | `time [0s,∞]$vt
◎初 `auto$v
◎適 すべての要素
◎継 されない
◎百 受容しない
◎算 指定されたとおり
◎順 文法に従う
◎表終

<p>
`voice-duration$p ~propは、
要素の内容が音声化される際に費やすべき時間を指定する
（［
`音声~cue$, `~pause$, `~rest$
］の時間は含まれない）。
値に `auto$v が指定されていない限り、
この~propは `voice-rate$p ~propより優先され，~voiceに適した発話速度の決定に利用されるべきである。
`voice-duration$p ~propの値が `auto$v でない要素の，子孫に指定されている［
`voice-duration$p, `voice-rate$p
］~propは、
無視するモノトスル。
すなわち，要素~上の
`voice-duration$p に指定された `time$t は、
要素の下位tree全体に適用される
（子はこの~propを上書きできない）。
◎
The voice-duration property specifies how long it should take to render the selected element’s content (not including audio cues, pauses and rests ). Unless the value auto is specified, this property takes precedence over the voice-rate property, and should be used to determine a suitable speaking rate for the voice. An element for which the voice-duration property value is not auto can have descendants for which the voice-duration and voice-rate properties are specified, but these must be ignored. In other words, when a &lt;time&gt; is specified for the voice-duration of a selected element, it applies to the entire element subtree (children cannot override the property).
</p>

<p class="note">注記：
この~propが供する機能性は、
`~SSML$の `prosody$sS 要素の `duration^a 属性に類似する。
◎
Note: The functionality provided by this property is similar to the duration attribute of the prosody element from the SSML markup language [SSML].
</p>

<dl class="valdef">
	<dt>`auto@v</dt>
	<dd>
継承される `voice-rate$p を利用している場合、
発話~合成の持続時間に対応する使用~値に解決される。
◎
Resolves to a used value corresponding to the duration of the speech synthesis when using the inherited voice-rate.
</dd>

	<dt id="valdef-voice-duration-time">`time [0s,∞]$t</dt>
	<dd>
絶対的な時間~単位（秒／~mmlli秒）による値を指定する
（例： `+3s^v, `250ms^v ）。
負な値は許容されない。
◎
Specifies a value in absolute time units (seconds and milliseconds, e.g. "+3s", "250ms"). Only non-negative values are allowed.
</dd>
</dl>

		</section>
	</section>
	<section id="lists">
<h2 title="List items and counters styles">13. ~list~itemと~counter~style</h2>

<p>
`list-style-type$p ~propは、［
~glyph, 付番~system, ~alphabet式~system
］いずれかによる`~list~item$用の~markerを指定する。
この~propに許容される値は、
`content$p ~propにおける `counter$f 関数にも利用される。
この~moduleは、
これらの~styleが`聴覚-次元$において発話~合成を利用してどの様に音声化されるかを定義する。
`list-style-image$p ~propは無視され，代わりに `list-style-type$p が利用される。
◎
The list-style-type property of [CSS2] specifies three types of list item markers: glyphs, numbering systems, and alphabetic systems. The values allowed for this property are also used for the counter() function of the content property. The CSS Speech module defines how to render these styles in the aural dimension, using speech synthesis. The list-style-image property of [CSS2] is ignored, and instead the list-style-type is used.
</p>

<p class="note">注記：
`CSS3LIST$r による新たな特能に対する発話~音声化は、
この~levelのこの仕様の対象外にあるが，将来の仕様にて定義され得る。
【 `CSS Counter Style^cite ~moduleにて`定義されている＠~CSSCOUNTER#counter-style-speak-as$。】
◎
Note: The speech rendering of new features from the CSS Lists and Counters Module Level 3 [CSS3LIST] is not covered in this level of CSS Speech, but may be defined in a future specification.
</p>

<dl>
	<dt>`disc^v</dt>
	<dt>`circle^v</dt>
	<dt>`square^v</dt>
	<dd>
これらの~list~item~styleに対しては、
~UAが［
どの等価な~phraseが発話されるか, あるいは
どの音声~cueが再生されるか
］を定義する
（場合によっては、
利用者の選好に基づいて）。
したがって，~graphicな~bullet（箇条書き記号）を伴う`~list~item$は、
実装に依存する方式で適切に発声される。
◎
For these list item styles, the user agent defines (possibly based on user preferences) what equivalent phrase is spoken or what audio cue is played. List items with graphical bullets are therefore announced appropriately in an implementation-dependent manner.
</dd>

	<dt>`decimal$v</dt>
	<dt>`decimal-leading-zero$v</dt>
	<dt>`lower-roman$v</dt>
	<dt>`upper-roman$v</dt>
	<dt>`georgian$v</dt>
	<dt>`armenian$v</dt>
	<dd>
これらの~list~item~styleに対しては、
対応する数がそのまま，発話~合成器により発話される。
また、
`~list~item$が在ることを指示するために，追加的な［
音声~cueまたは［
文書の自然言語による
（すなわち，~list~itemの内容を発話するものと同じ~TTS~voiceによる）
発話~phrase
］］も補われ得る。
例えば，~Englishが利用されているときは、
各~list~item~counterの前に単語 `Item^en が補われる結果，~list~itemは
`Item one^en, `Item two^en,
等々と発声されることになろう。
◎
For these list item styles, corresponding numbers are spoken as-is by the speech synthesizer, and may be complemented with additional audio cues or speech phrases in the document’s language (i.e. with the same TTS voice used to speak the list item content) in order to indicate the presence of list items. For example, when using the English language, the list item counter could be prefixed with the word "Item", which would result in list items being announced with "Item one", "Item two", etc.
</dd>

	<dt>`lower-latin$v</dt>
	<dt>`lower-alpha$v</dt>
	<dt>`upper-latin$v</dt>
	<dt>`upper-alpha$v</dt>
	<dt>`lower-greek$v</dt>
	<dd>
これらの~list~item~styleは、
文書の自然言語【`内容~言語$】の下で
（すなわち，`~list~item$の内容を発話するものと同じ~TTS~voiceを利用して），
発話~合成器により 一字ごとに読み綴られる。
例えば，~Englishの下では、
`lower-greek$v は［
`alpha^en, `beta^en, `gamma^en, 等々
］と読上げられることになろう。
類似に，~Frenchの下では、
`upper-latin$v は，音標による記法で［
<samp>/a/</samp>, <samp>/be/</samp>, <samp>/se/</samp>, 等々
］と読上げられることになろう。
◎
These list item styles are spelled out letter-by-letter by the speech synthesizer, in the document language (i.e. with the same TTS voice used to speak the list item content). For example, lower-greek in English would be read out as "alpha", "beta", "gamma", etc. Similarly, upper-latin in French would be read out as /a/, /be/, /se/, etc. (phonetic notation)
</dd>
</dl>

<p class="note">注記：
~screen-readerなどの~UAでは、
`~list~item$が入子にされた深さを発声したり、
より一般には，複階的な階層的~内容に伴われる構造-情報を追加で指示することが共通的に行われている。
この種の追加的な［
音声~cueや発話~出力
］の冗漫さは、
通例的には利用者により制御され，利用能を高める。
この種の~navi援助は実装に依存するが，
この~moduleを~supportする~UAには、
この種の音声~cueや発話~出力に際しては，［
冗長さは生成されない／一貫性は損なわれない
］ことを確保することが推奨される
（例えば、
同じ~listの~itemたちに複数の付番方式が~~適用されないようにするなど）。
◎
Note: It is common for user agents such as screen readers to announce the nesting depth of list items, or more generally, to indicate additional structural information pertaining to complex hierarchical content. The verbosity of these additional audio cues and/or speech output can usually be controlled by users, and contribute to increasing usability. These navigation aids are implementation-dependent, but it is recommended that user agents supporting the CSS Speech module ensure that these additional audio cues and speech output don’t generate redundancies or create inconsistencies (for example: duplicated or different list item numbering scheme).
</p>

	</section>
	<section id="content">
<h2 title="Inserted and replaced content">14. 挿入される内容と置換される内容</h2>

◎非規範的

<p>
定例の発音~規則の適用に先立って、
作者から，~source~textから別の文字列への対応付けを指定したいと求まれることもある。
これは、
共通的な略語や頭字語ではない，合成器からは認識されそうにないものに利用され得る。
`content$p ~propは、
文字列を別の文字列に置換するために利用できる。
この~propが供する機能性は、
`~SSML$の `sub$sS 要素の `alias^a 属性に類似することに注意。
◎
Sometimes, authors will want to specify a mapping from the source text into another string prior to the application of the regular pronunciation rules. This may be used for uncommon abbreviations or acronyms which are unlikely to be recognized by the synthesizer. The content property can be used to replace one string by another. The functionality provided by this property is similar to the alias attribute of the sub element from the SSML markup language [SSML].
</p>

<div class="example">
<p>
次の例では、
略語の音声化に際し，要素の内容の代わりに `title^a 属性の内容が利用される。
◎
In this example, the abbreviation is rendered using the content of the title attribute instead of the element’s content.
</p>

<pre class="lang-css">
    /* <span class="comment">
これは、
要素の内容を 文字列 "World Wide Web Consortium" に置換する。
◎
This replaces the content of the selected element by the string "World Wide Web Consortium".
</span> */
abbr { content: attr(title); }
</pre>
…
<pre class="lang-html">
&lt;abbr title="World Wide Web Consortium"&gt;W3C&lt;/abbr&gt;
</pre>

</div>

<p>
文書~内の~text文字列も，類似な仕方で以前に録音-済みな~versionに置換できる。
◎
In a similar way, text strings in a document can be replaced by a previously recorded version.
</p>

<div class="example">
<p>
次の例では、
`Sir John Gielgud^en による著名な~monologueの朗読の録音が再生される。
ここでは、
その形式は~supportされていて，~fileは可用であり，~UAはそうするよう環境設定されているとする
— 他の場合、
~UAは，合成した発話で~textを音声化するよう~fall-backすることになる。
◎
In this example—assuming the format is supported, the file is available and the UA is configured to do so—a recording of Sir John Gielgud’s declamation of the famous monologue is played. Otherwise the UA falls back to render the text using synthesized speech.
</p>

<pre class="lang-css">
.hamlet { content: url(./audio/gielgud.wav); }
</pre>
…
<pre class="lang-html">
&lt;div class="hamlet"&gt;
To be, or not to be: that is the question:
&lt;/div&gt;
</pre>

</div>

<p>
更に，作者は（または，利用者は利用者~stylesheetを介して）、
文書との視覚的でないヤリトリにおいても構造が理解し易くなるよう，
何らかの情報を追加してもヨイ。
それらは［
`before$pe ／ `after$pe
］疑似要素を利用して追加できる。
複数の~stylesheetを利用すれば、
~screen-readerから発話される追加的な情報の冗漫さを複数~level定義できることに注意。
◎
Furthermore, authors (or users via a user stylesheet) may add some information to ease the understanding of structures during non-visual interaction with the document. They can do so by using the ::before and ::after pseudo-elements. Note that different stylesheets can be used to define the level of verbosity for additional information spoken by screen readers.
</p>

<div class="example">
<p>
この例では、
~listの前に文字列
"箇条書き~~開始。"
を挿入し,
各`~list~item$の内容の前に文字列
"一つ、
"
を挿入する。
同様に、
~listの後には，利用者に~listの発話~出力を終えたことを伝えるための文字列
"箇条書き~~終了。"
が挿入される。
◎
This example inserts the string "Start list: " before a list and the string "List item: " before the content of each list item. Likewise, the string "List end: " gets inserted after the list to inform the user that the list speech output is over.
</p>

<pre class="lang-css">
ul::before { content: "箇条書き~~開始。"; }
ul::after  { content: "一つ、
"; }
li::before { content: "箇条書き~~終了。"; }
</pre>

<!-- 
ul::before { content: "Start list: "; }
ul::after  { content: "List end. "; }
li::before { content: "List item: "; }
 -->
</div>

<p>
詳細な情報は `CSS3GENCON$r に見出せる。
◎
Detailed information can be found in the CSS3 Generated and Replaced Content module [CSS3GENCON].
</p>

	</section>
	<section id="pronunciation">
<h2 title="Pronunciation, phonemes">15. 発音と音素</h2>

◎非規範的

<p>
~CSSは、
~markup文書~内の特定0の~text片の発音
（きちんと定義された音標~alphabetで表出される）
については，それを定義する方法を指定しない。
この仕様の以前の草案は， “音素” （ `phonemes^en ）~propについて述べていたが、
内容と呈示の分離の原則に抵触するとして，異論が提起された
（聴覚-~CSS~stylesheet内に著作された “音素” は、
~markup文書~内の~textが変更される度に更新される必要がある）。
したがって， “音素” の機能性は、
~CSS（呈示~層）の視野から外れると見なされており，［
~markup層／内容~層
］において取組まれるべきである。
◎
CSS does not specify how to define the pronunciation (expressed using a well-defined phonetic alphabet) of a particular piece of text within the markup document. A "phonemes" property was described in earlier drafts of this specification, but objections were raised due to breaking the principle of separation between content and presentation. (The "phonemes" authored within aural CSS stylesheets would have needed to be updated each time text changed within the markup document). The "phonemes" functionality is therefore considered out-of-scope in CSS (the presentation layer) and should be addressed in the markup / content layer.
</p>

<p>
`rel^a 値に "`pronunciation$v" を伴う `link$e 要素を利用すれば
（~CSS~stylesheetを含めるときと類似な方法で），
~HTML文書に発音~lexiconを取り込めるようになる。
`PRONUNCIATION-LEXICON$r は、
そのような~lexiconを述べるために利用できる形式の一つである。
◎
The "pronunciation" rel value allows importing pronunciation lexicons in HTML documents using the link element (similar to how CSS stylesheets can be included). The W3C PLS (Pronunciation Lexicon Specification) [PRONUNCIATION-LEXICON] is one format that can be used to describe such a lexicon.
</p>

<p>
加えて，~markup内では、
~textと発音の結付けの著作に，属性に基づく仕組みを利用できる。
そのような仕組みは、
この仕様が書かれた時点では，
【！W3C】~HTML標準において公式的に定義されてはいない。
しかしながら，
`EPUB 3.0 仕様＠http://idpf.org/epub/30$
では、
`~SSML$仕様から導出された［
~textを特定0の音標~alphabetに基づいて発音する方法
］を述べる属性を，~HTML文書に含めることが許容されている。
◎
Additionally, an attribute-based mechanism can be used within the markup to author text-pronunciation associations At the time of writing, such mechanism isn’t formally defined in the W3C HTML standard(s). However, the EPUB 3.0 specification allows (x)HTML5 documents to contain attributes derived from the [SSML] specification, that describe how to pronounce text based on a particular phonetic alphabet.
</p>

	</section>
<!-- 

Glossary

The following terms and abbreviations are used in this module.

UA
user agent

    A program that reads and/or writes CSS style sheets on behalf of a user in either or both of these categories: programs whose purpose is to render documents (e.g., browsers) and programs whose purpose is to create style sheets (e.g., editors). A UA may fall into both categories. (There are other programs that read or write style sheets, but this module gives no rules for them.) 
document

    A tree-structured document with elements and attributes, such as an SGML or XML document [XML11]. 
style sheet

    A CSS style sheet 
 -->

	<section id="ack">
<h2 title="Acknowledgements">謝辞</h2>

<p>
この仕様の策定を支援された［
W3C Voice Browser ~WG,
~CSS~WG
］の方々に感謝する。
詳細な~commentを寄せられた `Ellen Eide^en 氏（ IBM ），綿密に考査された `Elika Etemad^en 氏（ `Fantasai^en ）には特に感謝する。
◎
The editors would like to thank the members of the W3C Voice Browser and Cascading Style Sheets working groups for their assistance in preparing this specification. Special thanks to Ellen Eide (IBM) for her detailed comments, and to Elika Etemad (Fantasai) for her thorough reviews.
</p>

	</section>
	<section id="changes">
<h2 title="Changes">変更点</h2>

<dl>
	<dt>
`2012 年 勧告候補＠~TR/2012/CR-css3-speech-20120320/$
からの変更点
◎
The following changes have been made since the 2012 Candidate Recommendation:
</dt>
	<dd>
明確さを得るため、
`speak$p ~prop用の値［
`none^v ／ `normal^v
］を［
`never^v ／ `always^v
］に改称した。
（`510$issue）
◎
Renamed the none and normal values of speak to never and always for clarity. See Issue 510.
</dd>
	<dd>
`speak$p 用の値 `auto^v は、
`visibility$p に応答するようにした。
（`511$issue）
◎
Made the auto value of speak respond to visibility. See Issue 511.
</dd>
</dl>

<p>
加えて、
小さな編集上の修正点もある。
また、
~sourceを `Bikeshed^en 形式に変換した。
◎
In addition there have been some minor editorial fixes. and the source has been converted to Bikeshed format.
</p>

	</section>
</main></div>
